{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ffd16a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediciting Evolutionary Dynamics of Microbial Systems with  Reinforcement Learning and Dynamic Flux Balance Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d7212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overal View of the Algorithm\n",
    "\n",
    "![img](Process.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f1bae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: Define the Constants and Load The Toy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a56cc54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_BATCHES=100\n",
    "BATCH_SIZE=8\n",
    "HIDDEN_SIZE=30\n",
    "PERCENTILE=70\n",
    "\n",
    "import os \n",
    "import datetime\n",
    "import numpy as np\n",
    "import cobra\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import pandas\n",
    "#import cplex\n",
    "from ToyModel import  Toy_Model_NE_1,Toy_Model_NE_2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple\n",
    "import ray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorboardX import SummaryWriter\n",
    "from heapq import heappop, heappush"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b44a7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Then we have to define a number of classes, objects, and functions that will be used during the simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89f8f3ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Scaler=StandardScaler()\n",
    "CORES = multiprocessing.cpu_count()\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "Main_dir = os.path.dirname(\".\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class ProrityQueue:\n",
    "    \n",
    "    def __init__(self,N):\n",
    "        self.N=N\n",
    "        self.Elements=[]\n",
    "    \n",
    "    def enqueue_with_priority(self,Step):\n",
    "        Element = (-Step[0], random.random(),Step[1],Step[2])\n",
    "        heappush(self.Elements, Element)\n",
    "\n",
    "    def dequeue(self):\n",
    "        heappop(self.Elements)\n",
    "    \n",
    "    def balance(self):\n",
    "        while len(self.Elements)>=self.N:\n",
    "            self.dequeue()\n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcabd8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of DFBA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "269a9c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def dFBA(Models, Mapping_Dict: dict, Init_C:np.ndarray, Params:dict, t_span:list, dt=0.1)->list:\n",
    "    \"\"\"\n",
    "    This function calculates the concentration of each species\n",
    "    Models is a list of COBRA Model objects\n",
    "    Mapping_Dict is a dictionary of dictionaries\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    # Initializing the ODE Solver\n",
    "    ##############################################################\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    ##############################################################\n",
    "    # Solving the ODE\n",
    "    ##############################################################\n",
    "    for m in Models:\n",
    "        m.episode_reward=0\n",
    "        m.episode_steps=[]\n",
    "    \n",
    "    sol, t = odeFwdEuler(ODE_System, Init_C, dt,  Params,\n",
    "                         t_span, Models, Mapping_Dict)\n",
    "    \n",
    "    for m in Models:\n",
    "        m.Episode=Episode(reward=m.episode_reward, steps=m.episode_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return [m.Episode for m in Models]\n",
    "\n",
    "\n",
    "def ODE_System(C, t, Models, Mapping_Dict, Params, dt):\n",
    "    \"\"\"\n",
    "    This function calculates the differential equations for the system\n",
    "    Models is a list of COBRA Model objects\n",
    "    NOTE: this implementation of DFBA is compatible with RL framework\n",
    "    Given a policy it will genrate episodes. Policies can be either deterministic or stochastic\n",
    "    Differential Equations Are Formatted as follows:\n",
    "    [0]-Models[1]\n",
    "    [1]-Models[2]\n",
    "    []-...\n",
    "    [n-1]-Models[n]\n",
    "    [n]-Exc[1]\n",
    "    [n+1]-Exc[2]\n",
    "    []-...\n",
    "    [n+m-1]-Exc[m]\n",
    "    [n+m]-Starch\n",
    "    \"\"\"\n",
    "    C[C < 0] = 0\n",
    "    dCdt = np.zeros(C.shape)\n",
    "    Sols = list([0 for i in range(Models.__len__())])\n",
    "    for i,M in enumerate(Models):\n",
    "        \n",
    "        if random.random()<M.epsilon:\n",
    "\n",
    "            M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]*(1-M.epsilon)+np.random.uniform(low=-0.2, high=0.2,size=len(M.actions))*M.epsilon\n",
    "        \n",
    "        else:\n",
    "\n",
    "            M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]\n",
    "        \n",
    "        for index,item in enumerate(Mapping_Dict[\"Ex_sp\"]):\n",
    "            if Mapping_Dict['Mapping_Matrix'][index,i]!=-1:\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].upper_bound=20\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].lower_bound=-General_Uptake_Kinetics(C[index+len(Models)])\n",
    "                \n",
    "            \n",
    "        for index,flux in enumerate(M.actions):\n",
    "            M.a[index]=Flux_Clipper(M.reactions[M.actions[index]].lower_bound,M.a[index],M.reactions[M.actions[index]].upper_bound)\n",
    "            M.reactions[M.actions[index]].lower_bound=M.a[index]\n",
    "            # M.reactions[M.actions[index]].upper_bound=M.a[index]\n",
    "\n",
    "        Sols[i] = Models[i].optimize()\n",
    "\n",
    "        if Sols[i].status == 'infeasible':\n",
    "            Models[i].reward= -10\n",
    "            dCdt[i] = 0\n",
    "\n",
    "        else:\n",
    "            dCdt[i] += Sols[i].objective_value*C[i]\n",
    "            Models[i].reward =Sols[i].objective_value\n",
    "\n",
    "\n",
    "\n",
    "    ### Writing the balance equations\n",
    "\n",
    "    for i in range(Mapping_Dict[\"Mapping_Matrix\"].shape[0]):\n",
    "        for j in range(len(Models)):\n",
    "            if Mapping_Dict[\"Mapping_Matrix\"][i, j] != -1:\n",
    "                if Sols[j].status == 'infeasible':\n",
    "                    dCdt[i] = 0\n",
    "                else:\n",
    "                    dCdt[i+len(Models)] += Sols[j].fluxes.iloc[Mapping_Dict[\"Mapping_Matrix\"]\n",
    "                                                                    [i, j]]*C[j]\n",
    "\n",
    "\n",
    "    for m in Models:\n",
    "        m.episode_reward += m.reward\n",
    "        m.episode_steps.append(EpisodeStep(observation=C[m.observables], action=m.a))\n",
    "    \n",
    "    dCdt += np.array(Params[\"Dilution_Rate\"])*(Params[\"Inlet_C\"]-C)\n",
    "    \n",
    "    return dCdt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb1220",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now we need some utility functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69f08830",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Build_Mapping_Matrix(Models):\n",
    "    \"\"\"\n",
    "    Given a list of COBRA model objects, this function will build a mapping matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Ex_sp = []\n",
    "    Temp_Map={}\n",
    "    for model in Models:\n",
    "        \n",
    "        \n",
    "        if not hasattr(model,\"Biomass_Ind\"):\n",
    "            raise Exception(\"Models must have 'Biomass_Ind' attribute in order for the DFBA to work properly!\")\n",
    "        \n",
    "        \n",
    "        for Ex_rxn in model.exchanges :\n",
    "            if Ex_rxn!=model.reactions[model.Biomass_Ind]:\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id not in Ex_sp:\n",
    "                    Ex_sp.append(list(Ex_rxn.metabolites.keys())[0].id)\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id in Temp_Map.keys():\n",
    "                   Temp_Map[list(Ex_rxn.metabolites.keys())[0].id][model]=Ex_rxn\n",
    "                else:\n",
    "                     Temp_Map[list(Ex_rxn.metabolites.keys())[0].id]={model:Ex_rxn}\n",
    "\n",
    "    Mapping_Matrix = np.zeros((len(Ex_sp), len(Models)), dtype=int)\n",
    "    for i, id in enumerate(Ex_sp):\n",
    "        for j, model in enumerate(Models):\n",
    "            if model in Temp_Map[id].keys():\n",
    "                Mapping_Matrix[i, j] = model.reactions.index(Temp_Map[id][model].id)\n",
    "            else:\n",
    "                Mapping_Matrix[i, j] = -1\n",
    "    return {\"Ex_sp\": Ex_sp, \"Mapping_Matrix\": Mapping_Matrix}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def General_Uptake_Kinetics(Compound: float, Model=\"\"):\n",
    "    \"\"\"\n",
    "    This function calculates the rate of uptake of a compound in the reactor\n",
    "    ###It is just a simple imaginary model: Replace it with better model if necessary###\n",
    "    Compound Unit: mmol\n",
    "\n",
    "    \"\"\"\n",
    "    return 10*(Compound/(Compound+20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def odeFwdEuler(ODE_Function, ICs, dt, Params, t_span, Models, Mapping_Dict):\n",
    "    Integrator_Counter = 0\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    sol = np.zeros((len(t), len(ICs)))\n",
    "    sol[0] = ICs\n",
    "    for i in range(1, len(t)):\n",
    "        sol[i] = sol[i-1] + \\\n",
    "            ODE_Function(sol[i-1], t[i-1], Models, Mapping_Dict,\n",
    "                         Params, dt)*dt\n",
    "        Integrator_Counter += 1\n",
    "    return sol, t\n",
    "\n",
    "\n",
    "def Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict, Batch_Size=10,t_span=[0, 100], dt=0.1):\n",
    "\n",
    "\n",
    "    Init_C[list(Params[\"Env_States\"])] = [random.uniform(Range[0], Range[1]) for Range in Params[\"Env_States_Initial_Ranges\"]]\n",
    "    \n",
    "\n",
    "    \n",
    "    Batch_Episodes=[]\n",
    "    for BATCH in range(Batch_Size):\n",
    "        Batch_Episodes.append(dFBA.remote(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "        # Batch_Episodes.append(dFBA(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "\n",
    "    return(ray.get(Batch_Episodes))    \n",
    "\n",
    "    # return(Batch_Episodes)    \n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.FloatTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n",
    "\n",
    "def Flux_Clipper(Min,Number,Max):\n",
    "    return(min(max(Min,Number),Max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccf07c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We now define the high level main function that controls every part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a19e38c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def main(Models: list = [Toy_Model_NE_1.copy(), Toy_Model_NE_2.copy()], max_time: int = 100, Dil_Rate: float = 0.000000001, alpha: float = 0.01, Starting_Q: str = \"FBA\")->None:\n",
    "    \"\"\"\n",
    "    This is the main function for running dFBA.\n",
    "    The main requrement for working properly is\n",
    "    that the models use the same notation for the\n",
    "    same reactions.\n",
    "\n",
    "    Starting_Policy:\n",
    "\n",
    "    Defult --> Random: Initial Policy will be a random policy for all agents.\n",
    "    Otherwise --> a list of policies, pickle file addresses, for each agent.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Adding Agents info ###-----------------------------------------------------\n",
    "\n",
    "    # State dimensions in this RLDFBA variant include: [Agent1,...,Agentn, glucose,starch]\n",
    "    for i in range(len(Models)):\n",
    "        if not hasattr(Models[i], \"_name\"):\n",
    "            Models[i].NAME = \"Agent_\" + str(i)\n",
    "            print(f\"Agent {i} has been given a defult name\")\n",
    "        Models[i].solver.objective.name = \"_pfba_objective\"\n",
    "    # -------------------------------------------------------------------------------\n",
    "\n",
    "    # Mapping internal reactions to external reactions, and operational parameter\n",
    "    # setup ###-------------------------------------------------------------------\n",
    "\n",
    "    # For more information about the structure of the ODEs,see ODE_System function\n",
    "    # or the documentation.\n",
    "\n",
    "    Mapping_Dict = Build_Mapping_Matrix(Models)\n",
    "    Init_C = np.ones((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "    Inlet_C = np.zeros((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "\n",
    "    #Parameters that are use inside DFBA\n",
    "\n",
    "    Params = {\n",
    "        \"Dilution_Rate\": Dil_Rate,\n",
    "        \"Inlet_C\": Inlet_C,\n",
    "        \"Agents_Index\": [i for i in range(len(Models))],\n",
    "    }\n",
    "\n",
    "    #Define Agent attributes\n",
    "    Obs=[i for i in range(len(Models))]\n",
    "    Obs.extend([Mapping_Dict[\"Ex_sp\"].index(sp)+len(Models) for sp in Mapping_Dict[\"Ex_sp\"] if sp!='P' ])\n",
    "    for ind,m in enumerate(Models):\n",
    "        m.observables=Obs\n",
    "        m.actions=(Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"A\"),ind],Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"B\"),ind])\n",
    "        m.Policy=Net(len(m.observables), HIDDEN_SIZE, len(m.actions))\n",
    "        m.optimizer=optim.SGD(params=m.Policy.parameters(), lr=0.01)\n",
    "        m.Net_Obj=nn.MSELoss()\n",
    "        m.epsilon=0.05\n",
    "        \n",
    "    ### I Assume that the environment states are all observable. Env states will be stochastic\n",
    "    Params[\"Env_States\"]=Models[0].observables\n",
    "    Params[\"Env_States_Initial_Ranges\"]=[[0.1,0.1+0.00000001],[0.1,0.1+0.00000001],[100,100+0.00001],[1,0.001+0.00000000001],[1,0.001+0.00000000001]]\n",
    "    for i in range(len(Models)):\n",
    "        Init_C[i] = 0.001\n",
    "        #Models[i].solver = \"cplex\"\n",
    "    writer = SummaryWriter(comment=\"-DeepRLDFBA_NECOM\")\n",
    "    Outer_Counter = 0\n",
    "\n",
    "\n",
    "    for c in range(NUMBER_OF_BATCHES):\n",
    "        for m in Models:\n",
    "            m.epsilon=0.01+0.99/(np.exp(c/20))\n",
    "        Batch_Out=Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict,Batch_Size=BATCH_SIZE)\n",
    "        Batch_Out=list(map(list, zip(*Batch_Out)))\n",
    "        for index,Model in enumerate(Models):\n",
    "            obs_v, acts_v, reward_b, reward_m=filter_batch(Batch_Out[index], PERCENTILE)\n",
    "            Model.optimizer.zero_grad()\n",
    "            action_scores_v = Model.Policy(obs_v)\n",
    "            loss_v = Model.Net_Obj(action_scores_v, acts_v)\n",
    "            loss_v.backward()\n",
    "            Model.optimizer.step()\n",
    "            print(f\"{Model.NAME}\")\n",
    "            print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (c, loss_v.item(), reward_m, reward_b))\n",
    "\n",
    "            writer.add_scalar(f\"{Model.NAME} reward_mean\", reward_m, c)\n",
    "    \n",
    "    Time=datetime.datetime.now().strftime(\"%d_%m_%Y.%H_%M_%S\")\n",
    "    Results_Dir=os.path.join(Main_dir,\"Outputs\",str(Time))\n",
    "    os.mkdir(Results_Dir)\n",
    "    with open(os.path.join(Results_Dir,\"Models.pkl\"),'wb') as f:\n",
    "        pickle.dump(Models,f)\n",
    "    return Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a94ccd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finally it's time to run!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94a98600",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 has been given a defult name\n",
      "Agent 1 has been given a defult name\n",
      "Agent_0\n",
      "0: loss=0.023, reward_mean=-4977.3, reward_bound=-4891.8\n",
      "Agent_1\n",
      "0: loss=0.031, reward_mean=-4818.6, reward_bound=-4777.8\n",
      "Agent_0\n",
      "1: loss=0.018, reward_mean=-4498.6, reward_bound=-4360.8\n",
      "Agent_1\n",
      "1: loss=0.026, reward_mean=-4612.4, reward_bound=-4570.9\n",
      "Agent_0\n",
      "2: loss=0.018, reward_mean=-4313.7, reward_bound=-4270.0\n",
      "Agent_1\n",
      "2: loss=0.027, reward_mean=-4386.1, reward_bound=-4278.8\n",
      "Agent_0\n",
      "3: loss=0.014, reward_mean=-3877.3, reward_bound=-3791.8\n",
      "Agent_1\n",
      "3: loss=0.019, reward_mean=-4159.8, reward_bound=-4038.8\n",
      "Agent_0\n",
      "4: loss=0.013, reward_mean=-3558.7, reward_bound=-3398.9\n",
      "Agent_1\n",
      "4: loss=0.020, reward_mean=-3879.8, reward_bound=-3810.8\n",
      "Agent_0\n",
      "5: loss=0.009, reward_mean=-3307.3, reward_bound=-3208.8\n",
      "Agent_1\n",
      "5: loss=0.013, reward_mean=-3557.4, reward_bound=-3486.9\n",
      "Agent_0\n",
      "6: loss=0.007, reward_mean=-2981.0, reward_bound=-2929.8\n",
      "Agent_1\n",
      "6: loss=0.011, reward_mean=-3342.4, reward_bound=-3288.9\n",
      "Agent_0\n",
      "7: loss=0.007, reward_mean=-2663.5, reward_bound=-2560.8\n",
      "Agent_1\n",
      "7: loss=0.009, reward_mean=-3086.1, reward_bound=-3024.8\n",
      "Agent_0\n",
      "8: loss=0.008, reward_mean=-2384.9, reward_bound=-2318.9\n",
      "Agent_1\n",
      "8: loss=0.014, reward_mean=-2942.3, reward_bound=-2840.8\n",
      "Agent_0\n",
      "9: loss=0.007, reward_mean=-2202.4, reward_bound=-2159.9\n",
      "Agent_1\n",
      "9: loss=0.012, reward_mean=-2698.6, reward_bound=-2672.8\n",
      "Agent_0\n",
      "10: loss=0.005, reward_mean=-2022.3, reward_bound=-1989.8\n",
      "Agent_1\n",
      "10: loss=0.007, reward_mean=-2562.3, reward_bound=-2476.8\n",
      "Agent_0\n",
      "11: loss=0.007, reward_mean=-1718.7, reward_bound=-1677.0\n",
      "Agent_1\n",
      "11: loss=0.013, reward_mean=-2329.9, reward_bound=-2245.9\n",
      "Agent_0\n",
      "12: loss=0.004, reward_mean=-1624.8, reward_bound=-1571.8\n",
      "Agent_1\n",
      "12: loss=0.007, reward_mean=-2122.3, reward_bound=-2109.8\n",
      "Agent_0\n",
      "13: loss=0.003, reward_mean=-1421.0, reward_bound=-1343.8\n",
      "Agent_1\n",
      "13: loss=0.004, reward_mean=-1857.3, reward_bound=-1803.8\n",
      "Agent_0\n",
      "14: loss=0.004, reward_mean=-1278.7, reward_bound=-1229.9\n",
      "Agent_1\n",
      "14: loss=0.008, reward_mean=-1764.9, reward_bound=-1683.9\n",
      "Agent_0\n",
      "15: loss=0.002, reward_mean=-1113.5, reward_bound=-1063.8\n",
      "Agent_1\n",
      "15: loss=0.003, reward_mean=-1656.1, reward_bound=-1580.9\n",
      "Agent_0\n",
      "16: loss=0.004, reward_mean=-913.7, reward_bound=-849.9\n",
      "Agent_1\n",
      "16: loss=0.009, reward_mean=-1502.4, reward_bound=-1408.8\n",
      "Agent_0\n",
      "17: loss=0.002, reward_mean=-813.5, reward_bound=-730.8\n",
      "Agent_1\n",
      "17: loss=0.004, reward_mean=-1382.3, reward_bound=-1346.8\n",
      "Agent_0\n",
      "18: loss=0.005, reward_mean=-621.2, reward_bound=-620.0\n",
      "Agent_1\n",
      "18: loss=0.009, reward_mean=-1306.1, reward_bound=-1247.8\n",
      "Agent_0\n",
      "19: loss=0.001, reward_mean=-591.0, reward_bound=-551.8\n",
      "Agent_1\n",
      "19: loss=0.003, reward_mean=-1183.6, reward_bound=-1170.8\n",
      "Agent_0\n",
      "20: loss=0.003, reward_mean=-428.7, reward_bound=-362.9\n",
      "Agent_1\n",
      "20: loss=0.007, reward_mean=-1034.8, reward_bound=-992.8\n",
      "Agent_0\n",
      "21: loss=0.003, reward_mean=-311.2, reward_bound=-300.9\n",
      "Agent_1\n",
      "21: loss=0.006, reward_mean=-949.8, reward_bound=-950.8\n",
      "Agent_0\n",
      "22: loss=0.003, reward_mean=-253.7, reward_bound=-213.0\n",
      "Agent_1\n",
      "22: loss=0.007, reward_mean=-829.8, reward_bound=-743.8\n",
      "Agent_0\n",
      "23: loss=0.002, reward_mean=-146.1, reward_bound=-119.9\n",
      "Agent_1\n",
      "23: loss=0.004, reward_mean=-698.6, reward_bound=-671.8\n",
      "Agent_0\n",
      "24: loss=0.003, reward_mean=-74.9, reward_bound=-69.9\n",
      "Agent_1\n",
      "24: loss=0.006, reward_mean=-622.3, reward_bound=-570.8\n",
      "Agent_0\n",
      "25: loss=0.003, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "25: loss=0.006, reward_mean=-536.1, reward_bound=-471.8\n",
      "Agent_0\n",
      "26: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_1\n",
      "26: loss=0.002, reward_mean=-459.8, reward_bound=-449.8\n",
      "Agent_0\n",
      "27: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_1\n",
      "27: loss=0.003, reward_mean=-416.1, reward_bound=-380.9\n",
      "Agent_0\n",
      "28: loss=0.003, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "28: loss=0.007, reward_mean=-269.8, reward_bound=-261.8\n",
      "Agent_0\n",
      "29: loss=0.002, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "29: loss=0.004, reward_mean=-201.1, reward_bound=-151.8\n",
      "Agent_0\n",
      "30: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_1\n",
      "30: loss=0.003, reward_mean=-137.3, reward_bound=-129.8\n",
      "Agent_0\n",
      "31: loss=0.002, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "31: loss=0.004, reward_mean=-79.8, reward_bound=-69.8\n",
      "Agent_0\n",
      "32: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "32: loss=0.003, reward_mean=-12.3, reward_bound=0.2\n",
      "Agent_0\n",
      "33: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_1\n",
      "33: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "34: loss=0.002, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "34: loss=0.004, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "35: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "35: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "36: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "36: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "37: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "37: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "38: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_1\n",
      "38: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "39: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_1\n",
      "39: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "40: loss=0.002, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "40: loss=0.004, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "41: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "41: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "42: loss=0.002, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "42: loss=0.004, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "43: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "43: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "44: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "44: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "45: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "45: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "46: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "46: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "47: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "47: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "48: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "48: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "49: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "49: loss=0.003, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "50: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "50: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "51: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "51: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "52: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "52: loss=0.003, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "53: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "53: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "54: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "54: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "55: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "55: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "56: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "56: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "57: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "57: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "58: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "58: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "59: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "59: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "60: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "60: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "61: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "61: loss=0.002, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "62: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "62: loss=0.003, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "63: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "63: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "64: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "64: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "65: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "65: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "66: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "66: loss=0.001, reward_mean=0.2, reward_bound=0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent_0\n",
      "67: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "67: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "68: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "68: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "69: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "69: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "70: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "70: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "71: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "71: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "72: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "72: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "73: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "73: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "74: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "74: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "75: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "75: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "76: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "76: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "77: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "77: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "78: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "78: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "79: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "79: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "80: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "80: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "81: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "81: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "82: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "82: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "83: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "83: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "84: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "84: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "85: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "85: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "86: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "86: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "87: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "87: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "88: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "88: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "89: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "89: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "90: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "90: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "91: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "91: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "92: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "92: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "93: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "93: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "94: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "94: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "95: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "95: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "96: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "96: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "97: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "97: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "98: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "98: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "99: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "99: loss=0.000, reward_mean=0.2, reward_bound=0.2\n"
     ]
    }
   ],
   "source": [
    "Models=main([Toy_Model_NE_1.copy(),Toy_Model_NE_2.copy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55575a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
