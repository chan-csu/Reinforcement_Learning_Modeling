{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ffd16a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediciting Evolutionary Dynamics of Microbial Systems with  Reinforcement Learning and Dynamic Flux Balance Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d7212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overal View of the Algorithm\n",
    "\n",
    "![img](Process.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f1bae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: Define the Constants and Load The Toy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra import Model, Reaction, Metabolite\n",
    "\n",
    "\"\"\"\n",
    "A Toy Model is a Cobra Model with the following:\n",
    "\n",
    "Toy_Model_SA\n",
    "\n",
    "Reactions(NOT BALANCED):\n",
    "\n",
    "-> S  Substrate uptake\n",
    "S + ADP -> S_x + ATP  ATP production from catabolism\n",
    "ATP -> ADP ATP maintenance\n",
    "S_x + ATP -> X + ADP  Biomass production\n",
    "S_x + ATP -> Amylase + ADP  Amylase production\n",
    "Amylase -> Amylase Exchange\n",
    "X -> Biomass Out\n",
    "S_x + ADP -> P + ATP Metabolism stuff!\n",
    "P ->  Product release\n",
    "\n",
    "Metabolites:\n",
    "\n",
    "P  Product\n",
    "S  Substrate\n",
    "S_x  Internal metabolite\n",
    "X  Biomass\n",
    "ADP  \n",
    "ATP\n",
    "Amylase\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Toy_Model_NE_1:\n",
    "\n",
    "\n",
    "EX_S_sp1: S -> lowerBound',-10,'upperBound',0\n",
    "EX_A_sp1: A -> lowerBound',-100,'upperBound',100\n",
    "EX_B_sp1: B -> lowerBound',-100,'upperBound',100\n",
    "EX_P_sp1: P->  lowerBound',0,'upperBound',100\n",
    "R_1_sp1: S  + 2 adp  -> P + 2 atp ,'lowerBound',0,'upperBound',Inf\n",
    "R_2_sp1: P + atp  -> B  + adp 'lowerBound',0,'upperBound',Inf\n",
    "R_3_sp1: P + 3 atp  -> A + 3 adp ,'lowerBound',0,'upperBound',Inf\n",
    "R_4_sp1: 'atp -> adp  lowerBound',0,'upperBound',Inf\n",
    "OBJ_sp1: 3 A + 3 B + 5 atp  -> 5 adp + biomass_sp1 lowerBound',0,'upperBound',Inf\n",
    "Biomass_1 biomass_sp1  -> ','lowerBound',0,'upperBound',Inf,'objectiveCoef', 1);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Toy_Model_NE_2:\n",
    "\n",
    "\n",
    "EX_S_sp1: S -> lowerBound',-10,'upperBound',0\n",
    "EX_A_sp1: A -> lowerBound',-100,'upperBound',100\n",
    "EX_B_sp1: B -> lowerBound',-100,'upperBound',100\n",
    "EX_P_sp1: P->  lowerBound',0,'upperBound',100\n",
    "R_1_sp1: S  + 2 adp  -> P + 2 atp ,'lowerBound',0,'upperBound',Inf\n",
    "R_2_sp1: P + atp  -> B  + adp 'lowerBound',0,'upperBound',Inf\n",
    "R_3_sp1: P + 3 atp  -> A + 3 adp ,'lowerBound',0,'upperBound',Inf\n",
    "R_4_sp1: 'atp -> adp  lowerBound',0,'upperBound',Inf\n",
    "OBJ_sp1: 3 A + 3 B + 5 atp  -> 5 adp + biomass_sp1 lowerBound',0,'upperBound',Inf\n",
    "Biomass_1 biomass_sp1  -> ','lowerBound',0,'upperBound',Inf,'objectiveCoef', 1);\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Toy_Model_NE_1 = Model('Toy_1')\n",
    "\n",
    "### S_Uptake ###\n",
    "\n",
    "EX_S_sp1 = Reaction('EX_S_sp1')\n",
    "S = Metabolite('S', compartment='c')\n",
    "EX_S_sp1.add_metabolites({S: -1})\n",
    "EX_S_sp1.lower_bound = -10\n",
    "EX_S_sp1.upper_bound = 0\n",
    "Toy_Model_NE_1.add_reaction(EX_S_sp1)\n",
    "\n",
    "\n",
    "EX_A_sp1 = Reaction('EX_A_sp1')\n",
    "A = Metabolite('A', compartment='c')\n",
    "EX_A_sp1.add_metabolites({A: -1})\n",
    "EX_A_sp1.lower_bound = -100\n",
    "EX_A_sp1.upper_bound = 100\n",
    "Toy_Model_NE_1.add_reaction(EX_A_sp1)\n",
    "\n",
    "\n",
    "EX_B_sp1 = Reaction('EX_B_sp1')\n",
    "B = Metabolite('B', compartment='c')\n",
    "EX_B_sp1.add_metabolites({B: -1})\n",
    "EX_B_sp1.lower_bound = -100\n",
    "EX_B_sp1.upper_bound = 100\n",
    "Toy_Model_NE_1.add_reaction(EX_B_sp1)\n",
    "\n",
    "\n",
    "\n",
    "EX_P_sp1 = Reaction('EX_P_sp1')\n",
    "P = Metabolite('P', compartment='c')\n",
    "EX_P_sp1.add_metabolites({P:-1})\n",
    "EX_P_sp1.lower_bound = 0\n",
    "EX_P_sp1.upper_bound = 100\n",
    "Toy_Model_NE_1.add_reaction(EX_P_sp1)\n",
    "\n",
    "\n",
    "R_1_sp1 = Reaction('R_1_sp1')\n",
    "ADP = Metabolite('ADP', compartment='c')\n",
    "ATP = Metabolite('ATP', compartment='c')\n",
    "R_1_sp1.add_metabolites({ADP: -2, S: -1, P: 1, ATP: 2})\n",
    "R_1_sp1.lower_bound = 0\n",
    "R_1_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_1_sp1)\n",
    "\n",
    "\n",
    "R_2_sp1 = Reaction('R_2_sp1')\n",
    "R_2_sp1.add_metabolites({ADP: 1, P: -1, B: 1, ATP: -1})\n",
    "R_2_sp1.lower_bound = 0\n",
    "R_2_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_2_sp1)\n",
    "\n",
    "\n",
    "# R_3_sp1 = Reaction('R_3_sp1')\n",
    "# R_3_sp1.add_metabolites({ADP: 3, P: -1, A: 1, ATP: -3})\n",
    "# R_3_sp1.lower_bound = 0\n",
    "# R_3_sp1.upper_bound = 1000\n",
    "# Toy_Model_NE_1.add_reaction(R_3_sp1)\n",
    "\n",
    "\n",
    "\n",
    "R_4_sp1 = Reaction('R_4_sp1')\n",
    "R_4_sp1.add_metabolites({ADP:1 ,ATP: -1})\n",
    "R_4_sp1.lower_bound = 0\n",
    "R_4_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_4_sp1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OBJ_sp1 = Reaction(\"OBJ_sp1\")\n",
    "biomass_sp1 = Metabolite('biomass_sp1', compartment='c')\n",
    "OBJ_sp1.add_metabolites({ADP:5 ,ATP: -5,biomass_sp1:0.01,A:-3,B:-3})\n",
    "OBJ_sp1.lower_bound = 0\n",
    "OBJ_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(OBJ_sp1)\n",
    "\n",
    "Biomass_1 = Reaction(\"Biomass_1\")\n",
    "Biomass_1.add_metabolites({biomass_sp1:-1})\n",
    "Biomass_1.lower_bound = 0\n",
    "Biomass_1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(Biomass_1)\n",
    "\n",
    "Toy_Model_NE_1.objective='Biomass_1'\n",
    "Toy_Model_NE_1.Biomass_Ind=8\n",
    "\n",
    "\n",
    "### ADP Production From Catabolism ###\n",
    "\n",
    "Toy_Model_NE_2 = Model('Toy_2')\n",
    "\n",
    "### S_Uptake ###\n",
    "\n",
    "EX_S_sp2 = Reaction('EX_S_sp2')\n",
    "S = Metabolite('S', compartment='c')\n",
    "EX_S_sp2.add_metabolites({S: -1})\n",
    "EX_S_sp2.lower_bound = -10\n",
    "EX_S_sp2.upper_bound = 0\n",
    "Toy_Model_NE_2.add_reaction(EX_S_sp2)\n",
    "\n",
    "\n",
    "EX_A_sp2 = Reaction('EX_A_sp2')\n",
    "A = Metabolite('A', compartment='c')\n",
    "EX_A_sp2.add_metabolites({A: -1})\n",
    "EX_A_sp2.lower_bound = -100\n",
    "EX_A_sp2.upper_bound = 100\n",
    "Toy_Model_NE_2.add_reaction(EX_A_sp2)\n",
    "\n",
    "\n",
    "EX_B_sp2 = Reaction('EX_B_sp2')\n",
    "B = Metabolite('B', compartment='c')\n",
    "EX_B_sp2.add_metabolites({B: -1})\n",
    "EX_B_sp2.lower_bound = -100\n",
    "EX_B_sp2.upper_bound = 100\n",
    "Toy_Model_NE_2.add_reaction(EX_B_sp2)\n",
    "\n",
    "\n",
    "\n",
    "EX_P_sp2 = Reaction('EX_P_sp2')\n",
    "P = Metabolite('P', compartment='c')\n",
    "EX_P_sp2.add_metabolites({P:-1})\n",
    "EX_P_sp2.lower_bound = 0\n",
    "EX_P_sp2.upper_bound = 100\n",
    "Toy_Model_NE_2.add_reaction(EX_P_sp2)\n",
    "\n",
    "\n",
    "R_1_sp2 = Reaction('R_1_sp2')\n",
    "ADP = Metabolite('ADP', compartment='c')\n",
    "ATP = Metabolite('ATP', compartment='c')\n",
    "R_1_sp2.add_metabolites({ADP: -2, S: -1, P: 1, ATP: 2})\n",
    "R_1_sp2.lower_bound = 0\n",
    "R_1_sp2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(R_1_sp2)\n",
    "\n",
    "\n",
    "# R_2_sp2 = Reaction('R_2_sp2')\n",
    "# R_2_sp2.add_metabolites({ADP: 3, P: -1, B: 1, ATP: -3})\n",
    "# R_2_sp2.lower_bound = 0\n",
    "# R_2_sp2.upper_bound = 1000\n",
    "# Toy_Model_NE_2.add_reaction(R_2_sp2)\n",
    "\n",
    "\n",
    "R_3_sp2 = Reaction('R_3_sp2')\n",
    "R_3_sp2.add_metabolites({ADP: 1, P: -1, A: 1, ATP: -1})\n",
    "R_3_sp2.lower_bound = 0\n",
    "R_3_sp2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(R_3_sp2)\n",
    "\n",
    "\n",
    "\n",
    "R_4_sp2 = Reaction('R_4_sp2')\n",
    "R_4_sp2.add_metabolites({ADP:1 ,ATP: -1})\n",
    "R_4_sp2.lower_bound = 0\n",
    "R_4_sp2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(R_4_sp2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OBJ_sp2 = Reaction(\"OBJ_sp2\")\n",
    "biomass_sp2 = Metabolite('biomass_sp2', compartment='c')\n",
    "OBJ_sp2.add_metabolites({ADP:5 ,ATP: -5,biomass_sp2:0.01,A:-3,B:-3})\n",
    "OBJ_sp2.lower_bound = 0\n",
    "OBJ_sp2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(OBJ_sp2)\n",
    "\n",
    "Biomass_2 = Reaction(\"Biomass_2\")\n",
    "Biomass_2.add_metabolites({biomass_sp2:-1})\n",
    "Biomass_2.lower_bound = 0\n",
    "Biomass_2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(Biomass_2)\n",
    "Toy_Model_NE_2.objective=\"Biomass_2\"\n",
    "Toy_Model_NE_2.Biomass_Ind=8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(ToyModel_SA.optimize().fluxes)\n",
    "    print(ToyModel_SA.exchanges)\n",
    "    print(Toy_Model_NE_1.optimize().fluxes)\n",
    "    print(Toy_Model_NE_1.exchanges)\n",
    "    print(Toy_Model_NE_2.optimize().fluxes)\n",
    "    print(Toy_Model_NE_2.exchanges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8a56cc54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_BATCHES=1000\n",
    "BATCH_SIZE=8\n",
    "HIDDEN_SIZE=30\n",
    "PERCENTILE=70\n",
    "\n",
    "import os \n",
    "import datetime\n",
    "import numpy as np\n",
    "import cobra\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import pandas\n",
    "#import cplex\n",
    "from ToyModel import  Toy_Model_NE_1,Toy_Model_NE_2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple\n",
    "import ray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorboardX import SummaryWriter\n",
    "from heapq import heappop, heappush"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b44a7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Then we have to define a number of classes, objects, and functions that will be used during the simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "89f8f3ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Scaler=StandardScaler()\n",
    "CORES = multiprocessing.cpu_count()\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "Main_dir = os.path.dirname(\".\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class ProrityQueue:\n",
    "    \n",
    "    def __init__(self,N):\n",
    "        self.N=N\n",
    "        self.Elements=[]\n",
    "    \n",
    "    def enqueue_with_priority(self,Step):\n",
    "        Element = (-Step[0], random.random(),Step[1],Step[2])\n",
    "        heappush(self.Elements, Element)\n",
    "\n",
    "    def dequeue(self):\n",
    "        heappop(self.Elements)\n",
    "    \n",
    "    def balance(self):\n",
    "        while len(self.Elements)>=self.N:\n",
    "            self.dequeue()\n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcabd8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of DFBA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "269a9c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def dFBA(Models, Mapping_Dict: dict, Init_C:np.ndarray, Params:dict, t_span:list, dt=1)->list:\n",
    "    \"\"\"\n",
    "    This function calculates the concentration of each species\n",
    "    Models is a list of COBRA Model objects\n",
    "    Mapping_Dict is a dictionary of dictionaries\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    # Initializing the ODE Solver\n",
    "    ##############################################################\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    ##############################################################\n",
    "    # Solving the ODE\n",
    "    ##############################################################\n",
    "    for m in Models:\n",
    "        m.episode_reward=0\n",
    "        m.episode_steps=[]\n",
    "    \n",
    "    sol, t = odeFwdEuler(ODE_System, Init_C, dt,  Params,\n",
    "                         t_span, Models, Mapping_Dict)\n",
    "    \n",
    "    for i,m in enumerate(Models):\n",
    "        m.Episode=Episode(reward=m.episode_reward, steps=m.episode_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return [m.Episode for m in Models]\n",
    "\n",
    "\n",
    "def ODE_System(C, t, Models, Mapping_Dict, Params, dt):\n",
    "    \"\"\"\n",
    "    This function calculates the differential equations for the system\n",
    "    Models is a list of COBRA Model objects\n",
    "    NOTE: this implementation of DFBA is compatible with RL framework\n",
    "    Given a policy it will genrate episodes. Policies can be either deterministic or stochastic\n",
    "    Differential Equations Are Formatted as follows:\n",
    "    [0]-Models[1]\n",
    "    [1]-Models[2]\n",
    "    []-...\n",
    "    [n-1]-Models[n]\n",
    "    [n]-Exc[1]\n",
    "    [n+1]-Exc[2]\n",
    "    []-...\n",
    "    [n+m-1]-Exc[m]\n",
    "    [n+m]-Starch\n",
    "    \"\"\"\n",
    "    C[C < 0] = 0\n",
    "    dCdt = np.zeros(C.shape)\n",
    "    Sols = list([0 for i in range(Models.__len__())])\n",
    "    for i,M in enumerate(Models):\n",
    "        \n",
    "        if random.random()<M.epsilon:\n",
    "\n",
    "            M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]*(1-M.epsilon)+np.random.uniform(low=-5, high=5,size=len(M.actions))*M.epsilon\n",
    "        \n",
    "        else:\n",
    "\n",
    "            M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]\n",
    "        \n",
    "        for index,item in enumerate(Mapping_Dict[\"Ex_sp\"]):\n",
    "            if Mapping_Dict['Mapping_Matrix'][index,i]!=-1:\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].upper_bound=20\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].lower_bound=-General_Uptake_Kinetics(C[index+len(Models)])\n",
    "                \n",
    "            \n",
    "        for index,flux in enumerate(M.actions):\n",
    "            M.a[index]=Flux_Clipper(M.reactions[M.actions[index]].lower_bound,M.a[index],M.reactions[M.actions[index]].upper_bound)\n",
    "            M.reactions[M.actions[index]].lower_bound=M.a[index]\n",
    "            # M.reactions[M.actions[index]].upper_bound=M.a[index]\n",
    "\n",
    "        Sols[i] = Models[i].optimize()\n",
    "\n",
    "        if Sols[i].status == 'infeasible':\n",
    "            Models[i].reward= 0\n",
    "            dCdt[i] = 0\n",
    "\n",
    "        else:\n",
    "            dCdt[i] += Sols[i].objective_value*C[i]\n",
    "            Models[i].reward =Sols[i].objective_value\n",
    "\n",
    "\n",
    "\n",
    "    ### Writing the balance equations\n",
    "\n",
    "    for i in range(Mapping_Dict[\"Mapping_Matrix\"].shape[0]):\n",
    "        for j in range(len(Models)):\n",
    "            if Mapping_Dict[\"Mapping_Matrix\"][i, j] != -1:\n",
    "                if Sols[j].status == 'infeasible':\n",
    "                    dCdt[i] = 0\n",
    "                else:\n",
    "                    dCdt[i+len(Models)] += Sols[j].fluxes.iloc[Mapping_Dict[\"Mapping_Matrix\"]\n",
    "                                                                    [i, j]]*C[j]\n",
    "\n",
    "\n",
    "    for m in Models:\n",
    "        m.episode_reward += m.reward\n",
    "        m.episode_steps.append(EpisodeStep(observation=C[m.observables], action=m.a))\n",
    "    \n",
    "    dCdt += np.array(Params[\"Dilution_Rate\"])*(Params[\"Inlet_C\"]-C)\n",
    "    \n",
    "    return dCdt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb1220",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now we need some utility functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "69f08830",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Build_Mapping_Matrix(Models):\n",
    "    \"\"\"\n",
    "    Given a list of COBRA model objects, this function will build a mapping matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Ex_sp = []\n",
    "    Temp_Map={}\n",
    "    for model in Models:\n",
    "        \n",
    "        \n",
    "        if not hasattr(model,\"Biomass_Ind\"):\n",
    "            raise Exception(\"Models must have 'Biomass_Ind' attribute in order for the DFBA to work properly!\")\n",
    "        \n",
    "        \n",
    "        for Ex_rxn in model.exchanges :\n",
    "            if Ex_rxn!=model.reactions[model.Biomass_Ind]:\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id not in Ex_sp:\n",
    "                    Ex_sp.append(list(Ex_rxn.metabolites.keys())[0].id)\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id in Temp_Map.keys():\n",
    "                   Temp_Map[list(Ex_rxn.metabolites.keys())[0].id][model]=Ex_rxn\n",
    "                else:\n",
    "                     Temp_Map[list(Ex_rxn.metabolites.keys())[0].id]={model:Ex_rxn}\n",
    "\n",
    "    Mapping_Matrix = np.zeros((len(Ex_sp), len(Models)), dtype=int)\n",
    "    for i, id in enumerate(Ex_sp):\n",
    "        for j, model in enumerate(Models):\n",
    "            if model in Temp_Map[id].keys():\n",
    "                Mapping_Matrix[i, j] = model.reactions.index(Temp_Map[id][model].id)\n",
    "            else:\n",
    "                Mapping_Matrix[i, j] = -1\n",
    "    return {\"Ex_sp\": Ex_sp, \"Mapping_Matrix\": Mapping_Matrix}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def General_Uptake_Kinetics(Compound: float, Model=\"\"):\n",
    "    \"\"\"\n",
    "    This function calculates the rate of uptake of a compound in the reactor\n",
    "    ###It is just a simple imaginary model: Replace it with better model if necessary###\n",
    "    Compound Unit: mmol\n",
    "\n",
    "    \"\"\"\n",
    "    return 30*(Compound/(Compound+20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def odeFwdEuler(ODE_Function, ICs, dt, Params, t_span, Models, Mapping_Dict):\n",
    "    Integrator_Counter = 0\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    sol = np.zeros((len(t), len(ICs)))\n",
    "    sol[0] = ICs\n",
    "    for i in range(1, len(t)):\n",
    "        sol[i] = sol[i-1] + \\\n",
    "            ODE_Function(sol[i-1], t[i-1], Models, Mapping_Dict,\n",
    "                         Params, dt)*dt\n",
    "        Integrator_Counter += 1\n",
    "    return sol, t\n",
    "\n",
    "\n",
    "def Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict, Batch_Size=10,t_span=[0, 100], dt=0.1):\n",
    "\n",
    "\n",
    "    Init_C[list(Params[\"Env_States\"])] = [random.uniform(Range[0], Range[1]) for Range in Params[\"Env_States_Initial_Ranges\"]]\n",
    "    \n",
    "\n",
    "    \n",
    "    Batch_Episodes=[]\n",
    "    for BATCH in range(Batch_Size):\n",
    "        Batch_Episodes.append(dFBA.remote(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "        # Batch_Episodes.append(dFBA(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "\n",
    "    return(ray.get(Batch_Episodes))    \n",
    "\n",
    "    # return(Batch_Episodes)    \n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.FloatTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n",
    "\n",
    "def Flux_Clipper(Min,Number,Max):\n",
    "    return(min(max(Min,Number),Max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccf07c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We now define the high level main function that controls every part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4a19e38c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def main(Models: list = [Toy_Model_NE_1.copy(), Toy_Model_NE_2.copy()], max_time: int = 100, Dil_Rate: float = 0.000000001, alpha: float = 0.01, Starting_Q: str = \"FBA\")->None:\n",
    "    \"\"\"\n",
    "    This is the main function for running dFBA.\n",
    "    The main requrement for working properly is\n",
    "    that the models use the same notation for the\n",
    "    same reactions.\n",
    "\n",
    "    Starting_Policy:\n",
    "\n",
    "    Defult --> Random: Initial Policy will be a random policy for all agents.\n",
    "    Otherwise --> a list of policies, pickle file addresses, for each agent.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Adding Agents info ###-----------------------------------------------------\n",
    "\n",
    "    # State dimensions in this RLDFBA variant include: [Agent1,...,Agentn, glucose,starch]\n",
    "    for i in range(len(Models)):\n",
    "        if not hasattr(Models[i], \"_name\"):\n",
    "            Models[i].NAME = \"Agent_\" + str(i)\n",
    "            print(f\"Agent {i} has been given a defult name\")\n",
    "        Models[i].solver.objective.name = \"_pfba_objective\"\n",
    "    # -------------------------------------------------------------------------------\n",
    "\n",
    "    # Mapping internal reactions to external reactions, and operational parameter\n",
    "    # setup ###-------------------------------------------------------------------\n",
    "\n",
    "    # For more information about the structure of the ODEs,see ODE_System function\n",
    "    # or the documentation.\n",
    "\n",
    "    Mapping_Dict = Build_Mapping_Matrix(Models)\n",
    "    Init_C = np.ones((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "    Inlet_C = np.zeros((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "\n",
    "    #Parameters that are use inside DFBA\n",
    "\n",
    "    Params = {\n",
    "        \"Dilution_Rate\": Dil_Rate,\n",
    "        \"Inlet_C\": Inlet_C,\n",
    "        \"Agents_Index\": [i for i in range(len(Models))],\n",
    "    }\n",
    "\n",
    "    #Define Agent attributes\n",
    "    Obs=[i for i in range(len(Models))]\n",
    "    Obs.extend([Mapping_Dict[\"Ex_sp\"].index(sp)+len(Models) for sp in Mapping_Dict[\"Ex_sp\"] if sp!='P' ])\n",
    "    for ind,m in enumerate(Models):\n",
    "        m.observables=Obs\n",
    "        m.actions=(Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"A\"),ind],Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"B\"),ind])\n",
    "        m.Policy=Net(len(m.observables), HIDDEN_SIZE, len(m.actions))\n",
    "        m.optimizer=optim.SGD(params=m.Policy.parameters(), lr=0.01)\n",
    "        m.Net_Obj=nn.MSELoss()\n",
    "        m.epsilon=0.05\n",
    "        \n",
    "    ### I Assume that the environment states are all observable. Env states will be stochastic\n",
    "    Params[\"Env_States\"]=Models[0].observables\n",
    "    Params[\"Env_States_Initial_Ranges\"]=[[0.1,0.1+0.00000001],[0.1,0.1+0.00000001],[100,100+0.00001],[0.001,0.001+0.00000000001],[0.001,0.001+0.00000000001]]\n",
    "    for i in range(len(Models)):\n",
    "        Init_C[i] = 0.001\n",
    "        #Models[i].solver = \"cplex\"\n",
    "    writer = SummaryWriter(comment=\"-DeepRLDFBA_NECOM\")\n",
    "    Outer_Counter = 0\n",
    "\n",
    "\n",
    "    for c in range(NUMBER_OF_BATCHES):\n",
    "        for m in Models:\n",
    "            m.epsilon=0.01+0.99/(np.exp(c/20))\n",
    "        Batch_Out=Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict,Batch_Size=BATCH_SIZE)\n",
    "        Batch_Out=list(map(list, zip(*Batch_Out)))\n",
    "        for index,Model in enumerate(Models):\n",
    "            obs_v, acts_v, reward_b, reward_m=filter_batch(Batch_Out[index], PERCENTILE)\n",
    "            Model.optimizer.zero_grad()\n",
    "            action_scores_v = Model.Policy(obs_v)\n",
    "            loss_v = Model.Net_Obj(action_scores_v, acts_v)\n",
    "            loss_v.backward()\n",
    "            Model.optimizer.step()\n",
    "            print(f\"{Model.NAME}\")\n",
    "            print(\"%d: loss=%.3f, reward_mean=%.4f, reward_bound=%.4f\" % (c, loss_v.item(), reward_m, reward_b))\n",
    "\n",
    "            writer.add_scalar(f\"{Model.NAME} reward_mean\", reward_m, c)\n",
    "    \n",
    "    Time=datetime.datetime.now().strftime(\"%d_%m_%Y.%H_%M_%S\")\n",
    "    Results_Dir=os.path.join(Main_dir,\"Outputs\",str(Time))\n",
    "    os.mkdir(Results_Dir)\n",
    "    with open(os.path.join(Results_Dir,\"Models.pkl\"),'wb') as f:\n",
    "        pickle.dump(Models,f)\n",
    "    return Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a94ccd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finally it's time to run!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "94a98600",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 has been given a defult name\n",
      "Agent 1 has been given a defult name\n",
      "Agent_0\n",
      "0: loss=4.624, reward_mean=1.3279, reward_bound=1.3753\n",
      "Agent_1\n",
      "0: loss=5.169, reward_mean=1.5077, reward_bound=1.5842\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [169]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Models\u001b[38;5;241m=\u001b[39m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mToy_Model_NE_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mToy_Model_NE_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [168]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(Models, max_time, Dil_Rate, alpha, Starting_Q)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m Models:\n\u001b[1;32m     66\u001b[0m     m\u001b[38;5;241m.\u001b[39mepsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.99\u001b[39m\u001b[38;5;241m/\u001b[39m(np\u001b[38;5;241m.\u001b[39mexp(c\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m20\u001b[39m))\n\u001b[0;32m---> 67\u001b[0m Batch_Out\u001b[38;5;241m=\u001b[39m\u001b[43mGenerate_Batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdFBA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mInit_C\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMapping_Dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mBatch_Size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m Batch_Out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mBatch_Out)))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index,Model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Models):\n",
      "Input \u001b[0;32mIn [167]\u001b[0m, in \u001b[0;36mGenerate_Batch\u001b[0;34m(dFBA, Params, Init_C, Models, Mapping_Dict, Batch_Size, t_span, dt)\u001b[0m\n\u001b[1;32m     72\u001b[0m     Batch_Episodes\u001b[38;5;241m.\u001b[39mappend(dFBA\u001b[38;5;241m.\u001b[39mremote(Models, Mapping_Dict, Init_C, Params, t_span, dt\u001b[38;5;241m=\u001b[39mdt))\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Batch_Episodes.append(dFBA(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBatch_Episodes\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/lib/python3.9/site-packages/ray/worker.py:1825\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an object ref \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a list of object refs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     )\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 1825\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   1827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/lib/python3.9/site-packages/ray/worker.py:364\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m         )\n\u001b[1;32m    363\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 364\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data, metadata) \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1200\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:169\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Models=main([Toy_Model_NE_1.copy(),Toy_Model_NE_2.copy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16c5d2",
   "metadata": {},
   "source": [
    "## Step n: Plotting the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d87af8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observation space of each agent **in this particular example** is similar and contains 5 different states. Our goal is to create a meshgrid dataframe that includes these states and the actions taken as columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82225986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0d290",
   "metadata": {},
   "source": [
    "One way is to make a mesh grid which might be inefficient, probably is! So I won't complete it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c94f9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ag1=np.linspace(0,10,10)\n",
    "Ag2=np.linspace(0,10,10)\n",
    "S=np.linspace(0,100,10)\n",
    "A=np.linspace(0,10,10)\n",
    "B=np.linspace(0,10,10)\n",
    "Meshgrid=np.meshgrid(Ag1,Ag2,S,A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d110434",
   "metadata": {},
   "source": [
    "A much faster way is to generate a bunch of random points in the desired mesh and input them to the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3ee0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Space_Size=10000\n",
    "Space=np.random.uniform(low=[0,0,0,0,0],high=[10,10,100,10,10],size=(Space_Size,5))\n",
    "Actions_1=Models[0].Policy(torch.FloatTensor(Space))\n",
    "Actions_2=Models[1].Policy(torch.FloatTensor(Space))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7dbd7c00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DF_1=pd.DataFrame(np.hstack((Space,Actions_1.detach().numpy())),columns=[\"Agent1\",\"Agent2\",\"S\",\"A\",\"B\",\"A_Export\",\"B_Export\"])\n",
    "\n",
    "DF_2=pd.DataFrame(np.hstack((Space,Actions_2.detach().numpy())),columns=[\"Agent1\",\"Agent2\",\"S\",\"A\",\"B\",\"A_Export\",\"B_Export\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ac32d63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7139"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(DF_2[\"A_Export\"]<0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49492ba7",
   "metadata": {},
   "source": [
    "## Run a simulation with the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9e7f73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_t(Models: list = [Toy_Model_NE_1.copy(), Toy_Model_NE_2.copy()], max_time: int = 100, Dil_Rate: float = 0.000000001, alpha: float = 0.01, Starting_Q: str = \"FBA\"):\n",
    "    \"\"\"\n",
    "    This is the main function for running dFBA.\n",
    "    The main requrement for working properly is\n",
    "    that the models use the same notation for the\n",
    "    same reactions.\n",
    "\n",
    "    Starting_Policy:\n",
    "\n",
    "    Defult --> Random: Initial Policy will be a random policy for all agents.\n",
    "    Otherwise --> a list of policies, pickle file addresses, for each agent.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Mapping_Dict = Build_Mapping_Matrix(Models)\n",
    "    Init_C = np.ones((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "    Inlet_C = np.zeros((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "\n",
    "    #Parameters that are use inside DFBA\n",
    "\n",
    "    Params = {\n",
    "        \"Dilution_Rate\": Dil_Rate,\n",
    "        \"Inlet_C\": Inlet_C,\n",
    "        \"Agents_Index\": [i for i in range(len(Models))],\n",
    "    }\n",
    "\n",
    "    #Define Agent attributes\n",
    "\n",
    "    ### I Assume that the environment states are all observable. Env states will be stochastic\n",
    "    Params[\"Env_States\"]=Models[0].observables\n",
    "    Params[\"Env_States_Initial_Ranges\"]=[[0.1,0.100001],[0.1,0.100001],[50,50.001],[0.01,0.01+0.000001],[0.01,0.01+0.00001]]\n",
    "\n",
    "    Sol,t=Generate_Batch_t(dFBA, Params, Init_C, Models, Mapping_Dict)\n",
    "    return Sol,t\n",
    "    \n",
    "\n",
    "\n",
    "def dFBA_t(Models, Mapping_Dict, Init_C, Params, t_span, dt=0.1):\n",
    "    \"\"\"\n",
    "    This function calculates the concentration of each species\n",
    "    Models is a list of COBRA Model objects\n",
    "    Mapping_Dict is a dictionary of dictionaries\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    # Initializing the ODE Solver\n",
    "    ##############################################################\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    ##############################################################\n",
    "    # Solving the ODE\n",
    "    ##############################################################\n",
    "\n",
    "    \n",
    "    sol, t = odeFwdEuler(ODE_System_t, Init_C, dt,  Params,\n",
    "                         t_span, Models, Mapping_Dict)\n",
    "    \n",
    "    return sol,t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ODE_System_t(C, t, Models, Mapping_Dict, Params, dt):\n",
    "    \"\"\"\n",
    "    This function calculates the differential equations for the system\n",
    "    Models is a list of COBRA Model objects\n",
    "    NOTE: this implementation of DFBA is compatible with RL framework\n",
    "    Given a policy it will genrate episodes. Policies can be either deterministic or stochastic\n",
    "    Differential Equations Are Formatted as follows:\n",
    "    [0]-Models[1]\n",
    "    [1]-Models[2]\n",
    "    []-...\n",
    "    [n-1]-Models[n]\n",
    "    [n]-Exc[1]\n",
    "    [n+1]-Exc[2]\n",
    "    []-...\n",
    "    [n+m-1]-Exc[m]\n",
    "    [n+m]-Starch\n",
    "    \"\"\"\n",
    "    C[C < 0] = 0\n",
    "    dCdt = np.zeros(C.shape)\n",
    "    Sols = list([0 for i in range(Models.__len__())])\n",
    "    for i,M in enumerate(Models):\n",
    "        \n",
    "\n",
    "\n",
    "        M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]\n",
    "        \n",
    "        for index,item in enumerate(Mapping_Dict[\"Ex_sp\"]):\n",
    "            if Mapping_Dict['Mapping_Matrix'][index,i]!=-1:\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].upper_bound=20\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].lower_bound=-General_Uptake_Kinetics(C[index+len(Models)])\n",
    "                \n",
    "            \n",
    "        for index,flux in enumerate(M.actions):\n",
    "            M.a[index]=Flux_Clipper(M.reactions[flux].lower_bound,M.a[index],M.reactions[flux].upper_bound)\n",
    "            M.reactions[flux].lower_bound=M.a[index]\n",
    "            M.reactions[flux].upper_bound=M.a[index]\n",
    "\n",
    "        Sols[i] = Models[i].optimize()\n",
    "\n",
    "        if Sols[i].status == 'infeasible':\n",
    "            dCdt[i] = 0\n",
    "\n",
    "        else:\n",
    "            dCdt[i] += Sols[i].objective_value*C[i]\n",
    "            Models[i].reward =Sols[i].objective_value\n",
    "\n",
    "\n",
    "\n",
    "    ### Writing the balance equations\n",
    "\n",
    "    for i in range(Mapping_Dict[\"Mapping_Matrix\"].shape[0]):\n",
    "        for j in range(len(Models)):\n",
    "            if Mapping_Dict[\"Mapping_Matrix\"][i, j] != -1:\n",
    "                if Sols[j].status == 'infeasible':\n",
    "                    dCdt[i] = 0\n",
    "                else:\n",
    "                    dCdt[i+len(Models)] += Sols[j].fluxes.iloc[Mapping_Dict[\"Mapping_Matrix\"]\n",
    "                                                                    [i, j]]*C[j]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    dCdt += np.array(Params[\"Dilution_Rate\"])*(Params[\"Inlet_C\"]-C)\n",
    "    \n",
    "    return dCdt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Generate_Batch_t(dFBA, Params, Init_C, Models, Mapping_Dict,t_span=[0, 100], dt=1):\n",
    "\n",
    "\n",
    "    Init_C[list(Params[\"Env_States\"])] = [random.uniform(Range[0], Range[1]) for Range in Params[\"Env_States_Initial_Ranges\"]]\n",
    "    \n",
    "\n",
    "    Sol,t=dFBA_t(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt)\n",
    "        # Batch_Episodes.append(dFBA(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "\n",
    "    return Sol,t  \n",
    "\n",
    "\n",
    "Solution=main_t(Models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "db7c9200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x33033cd00>,\n",
       " <matplotlib.lines.Line2D at 0x33033cd60>,\n",
       " <matplotlib.lines.Line2D at 0x1771478b0>,\n",
       " <matplotlib.lines.Line2D at 0x1771470a0>,\n",
       " <matplotlib.lines.Line2D at 0x1771479a0>,\n",
       " <matplotlib.lines.Line2D at 0x177147910>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Solution[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780ee53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
