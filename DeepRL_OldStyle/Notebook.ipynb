{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ffd16a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediciting Evolutionary Dynamics of Microbial Systems with  Reinforcement Learning and Dynamic Flux Balance Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d7212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overal View of the Algorithm\n",
    "\n",
    "![img](Process.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f1bae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: Define the Constants and Load The Toy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a56cc54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling...\n",
      " A: min|aij| =  1.000e+00  max|aij| =  1.000e+00  ratio =  1.000e+00\n",
      "Problem data seem to be well scaled\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_BATCHES=100\n",
    "BATCH_SIZE=8\n",
    "HIDDEN_SIZE=30\n",
    "PERCENTILE=70\n",
    "\n",
    "import os \n",
    "import datetime\n",
    "import numpy as np\n",
    "import cobra\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import pandas\n",
    "#import cplex\n",
    "from ToyModel import  Toy_Model_NE_1,Toy_Model_NE_2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple\n",
    "import ray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorboardX import SummaryWriter\n",
    "from heapq import heappop, heappush"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b44a7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Then we have to define a number of classes, objects, and functions that will be used during the simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f8f3ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Scaler=StandardScaler()\n",
    "CORES = multiprocessing.cpu_count()\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "Main_dir = os.path.dirname(\".\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class ProrityQueue:\n",
    "    \n",
    "    def __init__(self,N):\n",
    "        self.N=N\n",
    "        self.Elements=[]\n",
    "    \n",
    "    def enqueue_with_priority(self,Step):\n",
    "        Element = (-Step[0], random.random(),Step[1],Step[2])\n",
    "        heappush(self.Elements, Element)\n",
    "\n",
    "    def dequeue(self):\n",
    "        heappop(self.Elements)\n",
    "    \n",
    "    def balance(self):\n",
    "        while len(self.Elements)>=self.N:\n",
    "            self.dequeue()\n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcabd8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of DFBA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269a9c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def dFBA(Models, Mapping_Dict: dict, Init_C:np.ndarray, Params:dict, t_span:list, dt=0.1)->list:\n",
    "    \"\"\"\n",
    "    This function calculates the concentration of each species\n",
    "    Models is a list of COBRA Model objects\n",
    "    Mapping_Dict is a dictionary of dictionaries\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    # Initializing the ODE Solver\n",
    "    ##############################################################\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    ##############################################################\n",
    "    # Solving the ODE\n",
    "    ##############################################################\n",
    "    for m in Models:\n",
    "        m.episode_reward=0\n",
    "        m.episode_steps=[]\n",
    "    \n",
    "    sol, t = odeFwdEuler(ODE_System, Init_C, dt,  Params,\n",
    "                         t_span, Models, Mapping_Dict)\n",
    "    \n",
    "    for m in Models:\n",
    "        m.Episode=Episode(reward=m.episode_reward, steps=m.episode_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return [m.Episode for m in Models]\n",
    "\n",
    "\n",
    "def ODE_System(C, t, Models, Mapping_Dict, Params, dt):\n",
    "    \"\"\"\n",
    "    This function calculates the differential equations for the system\n",
    "    Models is a list of COBRA Model objects\n",
    "    NOTE: this implementation of DFBA is compatible with RL framework\n",
    "    Given a policy it will genrate episodes. Policies can be either deterministic or stochastic\n",
    "    Differential Equations Are Formatted as follows:\n",
    "    [0]-Models[1]\n",
    "    [1]-Models[2]\n",
    "    []-...\n",
    "    [n-1]-Models[n]\n",
    "    [n]-Exc[1]\n",
    "    [n+1]-Exc[2]\n",
    "    []-...\n",
    "    [n+m-1]-Exc[m]\n",
    "    [n+m]-Starch\n",
    "    \"\"\"\n",
    "    C[C < 0] = 0\n",
    "    dCdt = np.zeros(C.shape)\n",
    "    Sols = list([0 for i in range(Models.__len__())])\n",
    "    for i,M in enumerate(Models):\n",
    "        \n",
    "        if random.random()<M.epsilon:\n",
    "\n",
    "            M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]*(1-M.epsilon)+np.random.uniform(low=-0.2, high=0.2,size=len(M.actions))*M.epsilon\n",
    "        \n",
    "        else:\n",
    "\n",
    "            M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]\n",
    "        \n",
    "        for index,item in enumerate(Mapping_Dict[\"Ex_sp\"]):\n",
    "            if Mapping_Dict['Mapping_Matrix'][index,i]!=-1:\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].upper_bound=20\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].lower_bound=-General_Uptake_Kinetics(C[index+len(Models)])\n",
    "                \n",
    "            \n",
    "        for index,flux in enumerate(M.actions):\n",
    "            M.a[index]=Flux_Clipper(M.reactions[M.actions[index]].lower_bound,M.a[index],M.reactions[M.actions[index]].upper_bound)\n",
    "            M.reactions[M.actions[index]].lower_bound=M.a[index]\n",
    "            # M.reactions[M.actions[index]].upper_bound=M.a[index]\n",
    "\n",
    "        Sols[i] = Models[i].optimize()\n",
    "\n",
    "        if Sols[i].status == 'infeasible':\n",
    "            Models[i].reward= -10\n",
    "            dCdt[i] = 0\n",
    "\n",
    "        else:\n",
    "            dCdt[i] += Sols[i].objective_value*C[i]\n",
    "            Models[i].reward =Sols[i].objective_value\n",
    "\n",
    "\n",
    "\n",
    "    ### Writing the balance equations\n",
    "\n",
    "    for i in range(Mapping_Dict[\"Mapping_Matrix\"].shape[0]):\n",
    "        for j in range(len(Models)):\n",
    "            if Mapping_Dict[\"Mapping_Matrix\"][i, j] != -1:\n",
    "                if Sols[j].status == 'infeasible':\n",
    "                    dCdt[i] = 0\n",
    "                else:\n",
    "                    dCdt[i+len(Models)] += Sols[j].fluxes.iloc[Mapping_Dict[\"Mapping_Matrix\"]\n",
    "                                                                    [i, j]]*C[j]\n",
    "\n",
    "\n",
    "    for m in Models:\n",
    "        m.episode_reward += m.reward\n",
    "        m.episode_steps.append(EpisodeStep(observation=C[m.observables], action=m.a))\n",
    "    \n",
    "    dCdt += np.array(Params[\"Dilution_Rate\"])*(Params[\"Inlet_C\"]-C)\n",
    "    \n",
    "    return dCdt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb1220",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now we need some utility functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f08830",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Build_Mapping_Matrix(Models):\n",
    "    \"\"\"\n",
    "    Given a list of COBRA model objects, this function will build a mapping matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Ex_sp = []\n",
    "    Temp_Map={}\n",
    "    for model in Models:\n",
    "        \n",
    "        \n",
    "        if not hasattr(model,\"Biomass_Ind\"):\n",
    "            raise Exception(\"Models must have 'Biomass_Ind' attribute in order for the DFBA to work properly!\")\n",
    "        \n",
    "        \n",
    "        for Ex_rxn in model.exchanges :\n",
    "            if Ex_rxn!=model.reactions[model.Biomass_Ind]:\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id not in Ex_sp:\n",
    "                    Ex_sp.append(list(Ex_rxn.metabolites.keys())[0].id)\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id in Temp_Map.keys():\n",
    "                   Temp_Map[list(Ex_rxn.metabolites.keys())[0].id][model]=Ex_rxn\n",
    "                else:\n",
    "                     Temp_Map[list(Ex_rxn.metabolites.keys())[0].id]={model:Ex_rxn}\n",
    "\n",
    "    Mapping_Matrix = np.zeros((len(Ex_sp), len(Models)), dtype=int)\n",
    "    for i, id in enumerate(Ex_sp):\n",
    "        for j, model in enumerate(Models):\n",
    "            if model in Temp_Map[id].keys():\n",
    "                Mapping_Matrix[i, j] = model.reactions.index(Temp_Map[id][model].id)\n",
    "            else:\n",
    "                Mapping_Matrix[i, j] = -1\n",
    "    return {\"Ex_sp\": Ex_sp, \"Mapping_Matrix\": Mapping_Matrix}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def General_Uptake_Kinetics(Compound: float, Model=\"\"):\n",
    "    \"\"\"\n",
    "    This function calculates the rate of uptake of a compound in the reactor\n",
    "    ###It is just a simple imaginary model: Replace it with better model if necessary###\n",
    "    Compound Unit: mmol\n",
    "\n",
    "    \"\"\"\n",
    "    return 10*(Compound/(Compound+20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def odeFwdEuler(ODE_Function, ICs, dt, Params, t_span, Models, Mapping_Dict):\n",
    "    Integrator_Counter = 0\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    sol = np.zeros((len(t), len(ICs)))\n",
    "    sol[0] = ICs\n",
    "    for i in range(1, len(t)):\n",
    "        sol[i] = sol[i-1] + \\\n",
    "            ODE_Function(sol[i-1], t[i-1], Models, Mapping_Dict,\n",
    "                         Params, dt)*dt\n",
    "        Integrator_Counter += 1\n",
    "    return sol, t\n",
    "\n",
    "\n",
    "def Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict, Batch_Size=10,t_span=[0, 100], dt=0.1):\n",
    "\n",
    "\n",
    "    Init_C[list(Params[\"Env_States\"])] = [random.uniform(Range[0], Range[1]) for Range in Params[\"Env_States_Initial_Ranges\"]]\n",
    "    \n",
    "\n",
    "    \n",
    "    Batch_Episodes=[]\n",
    "    for BATCH in range(Batch_Size):\n",
    "        Batch_Episodes.append(dFBA.remote(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "        # Batch_Episodes.append(dFBA(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "\n",
    "    return(ray.get(Batch_Episodes))    \n",
    "\n",
    "    # return(Batch_Episodes)    \n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.FloatTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n",
    "\n",
    "def Flux_Clipper(Min,Number,Max):\n",
    "    return(min(max(Min,Number),Max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccf07c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We now define the high level main function that controls every part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a19e38c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def main(Models: list = [Toy_Model_NE_1.copy(), Toy_Model_NE_2.copy()], max_time: int = 100, Dil_Rate: float = 0.000000001, alpha: float = 0.01, Starting_Q: str = \"FBA\")->None:\n",
    "    \"\"\"\n",
    "    This is the main function for running dFBA.\n",
    "    The main requrement for working properly is\n",
    "    that the models use the same notation for the\n",
    "    same reactions.\n",
    "\n",
    "    Starting_Policy:\n",
    "\n",
    "    Defult --> Random: Initial Policy will be a random policy for all agents.\n",
    "    Otherwise --> a list of policies, pickle file addresses, for each agent.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Adding Agents info ###-----------------------------------------------------\n",
    "\n",
    "    # State dimensions in this RLDFBA variant include: [Agent1,...,Agentn, glucose,starch]\n",
    "    for i in range(len(Models)):\n",
    "        if not hasattr(Models[i], \"_name\"):\n",
    "            Models[i].NAME = \"Agent_\" + str(i)\n",
    "            print(f\"Agent {i} has been given a defult name\")\n",
    "        Models[i].solver.objective.name = \"_pfba_objective\"\n",
    "    # -------------------------------------------------------------------------------\n",
    "\n",
    "    # Mapping internal reactions to external reactions, and operational parameter\n",
    "    # setup ###-------------------------------------------------------------------\n",
    "\n",
    "    # For more information about the structure of the ODEs,see ODE_System function\n",
    "    # or the documentation.\n",
    "\n",
    "    Mapping_Dict = Build_Mapping_Matrix(Models)\n",
    "    Init_C = np.ones((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "    Inlet_C = np.zeros((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "\n",
    "    #Parameters that are use inside DFBA\n",
    "\n",
    "    Params = {\n",
    "        \"Dilution_Rate\": Dil_Rate,\n",
    "        \"Inlet_C\": Inlet_C,\n",
    "        \"Agents_Index\": [i for i in range(len(Models))],\n",
    "    }\n",
    "\n",
    "    #Define Agent attributes\n",
    "    Obs=[i for i in range(len(Models))]\n",
    "    Obs.extend([Mapping_Dict[\"Ex_sp\"].index(sp)+len(Models) for sp in Mapping_Dict[\"Ex_sp\"] if sp!='P' ])\n",
    "    for ind,m in enumerate(Models):\n",
    "        m.observables=Obs\n",
    "        m.actions=(Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"A\"),ind],Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"B\"),ind])\n",
    "        m.Policy=Net(len(m.observables), HIDDEN_SIZE, len(m.actions))\n",
    "        m.optimizer=optim.SGD(params=m.Policy.parameters(), lr=0.01)\n",
    "        m.Net_Obj=nn.MSELoss()\n",
    "        m.epsilon=0.05\n",
    "        \n",
    "    ### I Assume that the environment states are all observable. Env states will be stochastic\n",
    "    Params[\"Env_States\"]=Models[0].observables\n",
    "    Params[\"Env_States_Initial_Ranges\"]=[[0.1,0.1+0.00000001],[0.1,0.1+0.00000001],[100,100+0.00001],[1,0.001+0.00000000001],[1,0.001+0.00000000001]]\n",
    "    for i in range(len(Models)):\n",
    "        Init_C[i] = 0.001\n",
    "        #Models[i].solver = \"cplex\"\n",
    "    writer = SummaryWriter(comment=\"-DeepRLDFBA_NECOM\")\n",
    "    Outer_Counter = 0\n",
    "\n",
    "\n",
    "    for c in range(NUMBER_OF_BATCHES):\n",
    "        for m in Models:\n",
    "            m.epsilon=0.01+0.99/(np.exp(c/20))\n",
    "        Batch_Out=Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict,Batch_Size=BATCH_SIZE)\n",
    "        Batch_Out=list(map(list, zip(*Batch_Out)))\n",
    "        for index,Model in enumerate(Models):\n",
    "            obs_v, acts_v, reward_b, reward_m=filter_batch(Batch_Out[index], PERCENTILE)\n",
    "            Model.optimizer.zero_grad()\n",
    "            action_scores_v = Model.Policy(obs_v)\n",
    "            loss_v = Model.Net_Obj(action_scores_v, acts_v)\n",
    "            loss_v.backward()\n",
    "            Model.optimizer.step()\n",
    "            print(f\"{Model.NAME}\")\n",
    "            print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (c, loss_v.item(), reward_m, reward_b))\n",
    "\n",
    "            writer.add_scalar(f\"{Model.NAME} reward_mean\", reward_m, c)\n",
    "    \n",
    "    Time=datetime.datetime.now().strftime(\"%d_%m_%Y.%H_%M_%S\")\n",
    "    Results_Dir=os.path.join(Main_dir,\"Outputs\",str(Time))\n",
    "    os.mkdir(Results_Dir)\n",
    "    with open(os.path.join(Results_Dir,\"Models.pkl\"),'wb') as f:\n",
    "        pickle.dump(Models,f)\n",
    "    return Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a94ccd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finally it's time to run!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a98600",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 has been given a defult name\n",
      "Agent 1 has been given a defult name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dFBA pid=37371)\u001b[0m /var/folders/jk/fr50qn391k794svhntbw333c0000gn/T/ipykernel_37349/47650564.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "\u001b[2m\u001b[36m(dFBA pid=37373)\u001b[0m /var/folders/jk/fr50qn391k794svhntbw333c0000gn/T/ipykernel_37349/47650564.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "\u001b[2m\u001b[36m(dFBA pid=37377)\u001b[0m /var/folders/jk/fr50qn391k794svhntbw333c0000gn/T/ipykernel_37349/47650564.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "\u001b[2m\u001b[36m(dFBA pid=37375)\u001b[0m /var/folders/jk/fr50qn391k794svhntbw333c0000gn/T/ipykernel_37349/47650564.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "\u001b[2m\u001b[36m(dFBA pid=37376)\u001b[0m /var/folders/jk/fr50qn391k794svhntbw333c0000gn/T/ipykernel_37349/47650564.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "\u001b[2m\u001b[36m(dFBA pid=37372)\u001b[0m /var/folders/jk/fr50qn391k794svhntbw333c0000gn/T/ipykernel_37349/47650564.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "\u001b[2m\u001b[36m(dFBA pid=37374)\u001b[0m /var/folders/jk/fr50qn391k794svhntbw333c0000gn/T/ipykernel_37349/47650564.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "\u001b[2m\u001b[36m(dFBA pid=37370)\u001b[0m /var/folders/jk/fr50qn391k794svhntbw333c0000gn/T/ipykernel_37349/47650564.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "/var/folders/jk/fr50qn391k794svhntbw333c0000gn/T/ipykernel_37349/2232768499.py:93: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  train_obs_v = torch.FloatTensor(train_obs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent_0\n",
      "0: loss=0.023, reward_mean=-4953.6, reward_bound=-4840.8\n",
      "Agent_1\n",
      "0: loss=0.032, reward_mean=-4859.8, reward_bound=-4730.8\n",
      "Agent_0\n",
      "1: loss=0.021, reward_mean=-4528.7, reward_bound=-4449.9\n",
      "Agent_1\n",
      "1: loss=0.030, reward_mean=-4599.8, reward_bound=-4504.8\n",
      "Agent_0\n",
      "2: loss=0.015, reward_mean=-4093.6, reward_bound=-3992.9\n",
      "Agent_1\n",
      "2: loss=0.023, reward_mean=-4392.4, reward_bound=-4281.9\n",
      "Agent_0\n",
      "3: loss=0.015, reward_mean=-3858.7, reward_bound=-3822.9\n",
      "Agent_1\n",
      "3: loss=0.021, reward_mean=-4049.8, reward_bound=-3968.8\n",
      "Agent_0\n",
      "4: loss=0.011, reward_mean=-3606.0, reward_bound=-3584.8\n",
      "Agent_1\n",
      "4: loss=0.015, reward_mean=-3656.1, reward_bound=-3607.9\n",
      "Agent_0\n",
      "5: loss=0.012, reward_mean=-3266.2, reward_bound=-3217.9\n",
      "Agent_1\n",
      "5: loss=0.019, reward_mean=-3584.8, reward_bound=-3529.8\n",
      "Agent_0\n",
      "6: loss=0.009, reward_mean=-2884.9, reward_bound=-2756.9\n",
      "Agent_1\n",
      "6: loss=0.014, reward_mean=-3393.6, reward_bound=-3328.8\n",
      "Agent_0\n",
      "7: loss=0.008, reward_mean=-2683.6, reward_bound=-2627.9\n",
      "Agent_1\n",
      "7: loss=0.012, reward_mean=-3154.8, reward_bound=-3100.8\n",
      "Agent_0\n",
      "8: loss=0.007, reward_mean=-2484.9, reward_bound=-2400.9\n",
      "Agent_1\n",
      "8: loss=0.011, reward_mean=-2891.1, reward_bound=-2862.8\n",
      "Agent_0\n",
      "9: loss=0.006, reward_mean=-2269.9, reward_bound=-2229.9\n",
      "Agent_1\n",
      "9: loss=0.011, reward_mean=-2709.9, reward_bound=-2685.9\n",
      "Agent_0\n",
      "10: loss=0.005, reward_mean=-2041.1, reward_bound=-1990.8\n",
      "Agent_1\n",
      "10: loss=0.007, reward_mean=-2531.1, reward_bound=-2464.8\n",
      "Agent_0\n",
      "11: loss=0.005, reward_mean=-1814.9, reward_bound=-1755.9\n",
      "Agent_1\n",
      "11: loss=0.009, reward_mean=-2413.6, reward_bound=-2315.9\n",
      "Agent_0\n",
      "12: loss=0.003, reward_mean=-1623.5, reward_bound=-1604.8\n",
      "Agent_1\n",
      "12: loss=0.004, reward_mean=-2151.1, reward_bound=-2091.8\n",
      "Agent_0\n",
      "13: loss=0.004, reward_mean=-1467.4, reward_bound=-1430.9\n",
      "Agent_1\n",
      "13: loss=0.008, reward_mean=-1918.6, reward_bound=-1919.8\n",
      "Agent_0\n",
      "14: loss=0.006, reward_mean=-1238.7, reward_bound=-1148.0\n",
      "Agent_1\n",
      "14: loss=0.011, reward_mean=-1804.8, reward_bound=-1761.8\n",
      "Agent_0\n",
      "15: loss=0.002, reward_mean=-1146.0, reward_bound=-1090.7\n",
      "Agent_1\n",
      "15: loss=0.003, reward_mean=-1677.3, reward_bound=-1641.8\n",
      "Agent_0\n",
      "16: loss=0.002, reward_mean=-1029.8, reward_bound=-990.8\n",
      "Agent_1\n",
      "16: loss=0.003, reward_mean=-1608.6, reward_bound=-1589.8\n",
      "Agent_0\n",
      "17: loss=0.003, reward_mean=-808.6, reward_bound=-789.9\n",
      "Agent_1\n",
      "17: loss=0.005, reward_mean=-1471.1, reward_bound=-1400.9\n",
      "Agent_0\n",
      "18: loss=0.003, reward_mean=-677.4, reward_bound=-617.8\n",
      "Agent_1\n",
      "18: loss=0.006, reward_mean=-1347.3, reward_bound=-1342.9\n",
      "Agent_0\n",
      "19: loss=0.002, reward_mean=-571.1, reward_bound=-539.8\n",
      "Agent_1\n",
      "19: loss=0.004, reward_mean=-1147.3, reward_bound=-1094.8\n",
      "Agent_0\n",
      "20: loss=0.002, reward_mean=-432.4, reward_bound=-412.8\n",
      "Agent_1\n",
      "20: loss=0.005, reward_mean=-1068.6, reward_bound=-1022.8\n",
      "Agent_0\n",
      "21: loss=0.003, reward_mean=-344.9, reward_bound=-341.9\n",
      "Agent_1\n",
      "21: loss=0.007, reward_mean=-928.6, reward_bound=-911.8\n",
      "Agent_0\n",
      "22: loss=0.003, reward_mean=-247.4, reward_bound=-230.9\n",
      "Agent_1\n",
      "22: loss=0.006, reward_mean=-799.8, reward_bound=-751.8\n",
      "Agent_0\n",
      "23: loss=0.003, reward_mean=-143.7, reward_bound=-120.9\n",
      "Agent_1\n",
      "23: loss=0.006, reward_mean=-687.3, reward_bound=-659.8\n",
      "Agent_0\n",
      "24: loss=0.003, reward_mean=-68.7, reward_bound=-70.9\n",
      "Agent_1\n",
      "24: loss=0.006, reward_mean=-646.1, reward_bound=-609.9\n",
      "Agent_0\n",
      "25: loss=0.003, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "25: loss=0.007, reward_mean=-521.1, reward_bound=-501.8\n",
      "Agent_0\n",
      "26: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "26: loss=0.003, reward_mean=-473.6, reward_bound=-451.8\n",
      "Agent_0\n",
      "27: loss=0.002, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "27: loss=0.004, reward_mean=-367.3, reward_bound=-359.8\n",
      "Agent_0\n",
      "28: loss=0.003, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "28: loss=0.006, reward_mean=-286.1, reward_bound=-270.8\n",
      "Agent_0\n",
      "29: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_1\n",
      "29: loss=0.002, reward_mean=-202.3, reward_bound=-191.8\n",
      "Agent_0\n",
      "30: loss=0.002, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "30: loss=0.005, reward_mean=-168.6, reward_bound=-124.8\n",
      "Agent_0\n",
      "31: loss=0.002, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "31: loss=0.004, reward_mean=-72.3, reward_bound=-59.8\n",
      "Agent_0\n",
      "32: loss=0.002, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "32: loss=0.005, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "33: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "33: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "34: loss=0.003, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "34: loss=0.006, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "35: loss=0.002, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "35: loss=0.004, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "36: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "36: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "37: loss=0.002, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "37: loss=0.005, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "38: loss=0.002, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "38: loss=0.004, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "39: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "39: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "40: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "40: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "41: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_1\n",
      "41: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "42: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "42: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "43: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "43: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "44: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "44: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "45: loss=0.002, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "45: loss=0.004, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "46: loss=0.002, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "46: loss=0.004, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "47: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "47: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "48: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "48: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "49: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "49: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "50: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "50: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "51: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "51: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "52: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "52: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "53: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "53: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "54: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "54: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "55: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "55: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "56: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "56: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "57: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "57: loss=0.003, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "58: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "58: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "59: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "59: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "60: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "60: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "61: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "61: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "62: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "62: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "63: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "63: loss=0.002, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "64: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "64: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "65: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "65: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "66: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "66: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "67: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "67: loss=0.002, reward_mean=0.2, reward_bound=0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent_0\n",
      "68: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "68: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "69: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "69: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "70: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "70: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "71: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "71: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "72: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "72: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "73: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "73: loss=0.002, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "74: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "74: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "75: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "75: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "76: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "76: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "77: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "77: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "78: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "78: loss=0.001, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "79: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "79: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "80: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "80: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "81: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "81: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "82: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "82: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "83: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "83: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "84: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "84: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "85: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "85: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "86: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "86: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "87: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "87: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "88: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "88: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "89: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "89: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_0\n",
      "90: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "90: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "91: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "91: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "92: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "92: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "93: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "93: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "94: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "94: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "95: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "95: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "96: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "96: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "97: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "Agent_1\n",
      "97: loss=0.001, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "98: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "98: loss=0.000, reward_mean=0.2, reward_bound=0.2\n",
      "Agent_0\n",
      "99: loss=0.000, reward_mean=0.1, reward_bound=0.1\n",
      "Agent_1\n",
      "99: loss=0.000, reward_mean=0.2, reward_bound=0.2\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "Models=main([Toy_Model_NE_1.copy(),Toy_Model_NE_2.copy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab34eb6",
   "metadata": {},
   "source": [
    "## Step n: Plotting the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f67d72",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observation space of each agent **in this particular example** is similar and contains 5 different states. Our goal is to create a meshgrid dataframe that includes these states and the actions taken as columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb1be75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eec5b6",
   "metadata": {},
   "source": [
    "One way is to make a mesh grid which might be inefficient, probably is! So I won't complete it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4842eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ag1=np.linspace(0,10,10)\n",
    "Ag2=np.linspace(0,10,10)\n",
    "S=np.linspace(0,100,10)\n",
    "A=np.linspace(0,10,10)\n",
    "B=np.linspace(0,10,10)\n",
    "Meshgrid=np.meshgrid(Ag1,Ag2,S,A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb211f0",
   "metadata": {},
   "source": [
    "A much faster way is to generate a bunch of random points in the desired mesh and input them to the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e3b9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Space_Size=10000\n",
    "Space=np.random.uniform(low=[0,0,0,0,0],high=[10,10,100,10,10],size=(Space_Size,5))\n",
    "Actions_1=Models[0].Policy(torch.FloatTensor(Space))\n",
    "Actions_2=Models[1].Policy(torch.FloatTensor(Space))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d17afab2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(pd.DataFrame(np.hstack((Space,Actions_1.detach().numpy()))).corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160611c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
