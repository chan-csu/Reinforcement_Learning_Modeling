{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ffd16a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediciting Evolutionary Dynamics of Microbial Systems with  Reinforcement Learning and Dynamic Flux Balance Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d7212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overal View of the Algorithm\n",
    "\n",
    "![img](Process.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f1bae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: Define toy models, and the constants and load the toy networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5733f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EX_S_sp1    -10.00\n",
      "EX_A_sp1    -12.00\n",
      "EX_B_sp1    -12.00\n",
      "EX_P_sp1     10.00\n",
      "R_1_sp1      10.00\n",
      "R_2_sp1       0.00\n",
      "R_3_sp1       0.00\n",
      "R_4_sp1       0.00\n",
      "OBJ_sp1       4.00\n",
      "Biomass_1     0.04\n",
      "Name: fluxes, dtype: float64\n",
      "[<Reaction EX_S_sp1 at 0x347e10e50>, <Reaction EX_A_sp1 at 0x366e4a1f0>, <Reaction EX_B_sp1 at 0x366e4a5b0>, <Reaction EX_P_sp1 at 0x366e4ad00>, <Reaction Biomass_1 at 0x317ae09a0>]\n",
      "EX_S_sp2    -10.00\n",
      "EX_A_sp2    -12.00\n",
      "EX_B_sp2    -12.00\n",
      "EX_P_sp2     10.00\n",
      "R_1_sp2      10.00\n",
      "R_2_sp2       0.00\n",
      "R_3_sp2       0.00\n",
      "R_4_sp2       0.00\n",
      "OBJ_sp2       4.00\n",
      "Biomass_2     0.04\n",
      "Name: fluxes, dtype: float64\n",
      "[<Reaction EX_S_sp2 at 0x317ae01f0>, <Reaction EX_A_sp2 at 0x32c06b7f0>, <Reaction EX_B_sp2 at 0x32c06b9d0>, <Reaction EX_P_sp2 at 0x32c06ba90>, <Reaction Biomass_2 at 0x317affeb0>]\n"
     ]
    }
   ],
   "source": [
    "from cobra import Model, Reaction, Metabolite\n",
    "\n",
    "\"\"\"\n",
    "A Toy Model is a Cobra Model with the following:\n",
    "\n",
    "Toy_Model_SA\n",
    "\n",
    "Reactions(NOT BALANCED):\n",
    "\n",
    "-> S  Substrate uptake\n",
    "S + ADP -> S_x + ATP  ATP production from catabolism\n",
    "ATP -> ADP ATP maintenance\n",
    "S_x + ATP -> X + ADP  Biomass production\n",
    "S_x + ATP -> Amylase + ADP  Amylase production\n",
    "Amylase -> Amylase Exchange\n",
    "X -> Biomass Out\n",
    "S_x + ADP -> P + ATP Metabolism stuff!\n",
    "P ->  Product release\n",
    "\n",
    "Metabolites:\n",
    "\n",
    "P  Product\n",
    "S  Substrate\n",
    "S_x  Internal metabolite\n",
    "X  Biomass\n",
    "ADP  \n",
    "ATP\n",
    "Amylase\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Toy_Model_NE_1:\n",
    "\n",
    "\n",
    "EX_S_sp1: S -> lowerBound',-10,'upperBound',0\n",
    "EX_A_sp1: A -> lowerBound',-100,'upperBound',100\n",
    "EX_B_sp1: B -> lowerBound',-100,'upperBound',100\n",
    "EX_P_sp1: P->  lowerBound',0,'upperBound',100\n",
    "R_1_sp1: S  + 2 adp  -> P + 2 atp ,'lowerBound',0,'upperBound',Inf\n",
    "R_2_sp1: P + atp  -> B  + adp 'lowerBound',0,'upperBound',Inf\n",
    "R_3_sp1: P + 3 atp  -> A + 3 adp ,'lowerBound',0,'upperBound',Inf\n",
    "R_4_sp1: 'atp -> adp  lowerBound',0,'upperBound',Inf\n",
    "OBJ_sp1: 3 A + 3 B + 5 atp  -> 5 adp + biomass_sp1 lowerBound',0,'upperBound',Inf\n",
    "Biomass_1 biomass_sp1  -> ','lowerBound',0,'upperBound',Inf,'objectiveCoef', 1);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Toy_Model_NE_2:\n",
    "\n",
    "\n",
    "EX_S_sp1: S -> lowerBound',-10,'upperBound',0\n",
    "EX_A_sp1: A -> lowerBound',-100,'upperBound',100\n",
    "EX_B_sp1: B -> lowerBound',-100,'upperBound',100\n",
    "EX_P_sp1: P->  lowerBound',0,'upperBound',100\n",
    "R_1_sp1: S  + 2 adp  -> P + 2 atp ,'lowerBound',0,'upperBound',Inf\n",
    "R_2_sp1: P + atp  -> B  + adp 'lowerBound',0,'upperBound',Inf\n",
    "R_3_sp1: P + 3 atp  -> A + 3 adp ,'lowerBound',0,'upperBound',Inf\n",
    "R_4_sp1: 'atp -> adp  lowerBound',0,'upperBound',Inf\n",
    "OBJ_sp1: 3 A + 3 B + 5 atp  -> 5 adp + biomass_sp1 lowerBound',0,'upperBound',Inf\n",
    "Biomass_1 biomass_sp1  -> ','lowerBound',0,'upperBound',Inf,'objectiveCoef', 1);\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Toy_Model_NE_1 = Model('Toy_1')\n",
    "\n",
    "### S_Uptake ###\n",
    "\n",
    "EX_S_sp1 = Reaction('EX_S_sp1')\n",
    "S = Metabolite('S', compartment='c')\n",
    "EX_S_sp1.add_metabolites({S: -1})\n",
    "EX_S_sp1.lower_bound = -10\n",
    "EX_S_sp1.upper_bound = 0\n",
    "Toy_Model_NE_1.add_reaction(EX_S_sp1)\n",
    "\n",
    "\n",
    "EX_A_sp1 = Reaction('EX_A_sp1')\n",
    "A = Metabolite('A', compartment='c')\n",
    "EX_A_sp1.add_metabolites({A: -1})\n",
    "EX_A_sp1.lower_bound = -100\n",
    "EX_A_sp1.upper_bound = 100\n",
    "Toy_Model_NE_1.add_reaction(EX_A_sp1)\n",
    "\n",
    "\n",
    "EX_B_sp1 = Reaction('EX_B_sp1')\n",
    "B = Metabolite('B', compartment='c')\n",
    "EX_B_sp1.add_metabolites({B: -1})\n",
    "EX_B_sp1.lower_bound = -100\n",
    "EX_B_sp1.upper_bound = 100\n",
    "Toy_Model_NE_1.add_reaction(EX_B_sp1)\n",
    "\n",
    "\n",
    "\n",
    "EX_P_sp1 = Reaction('EX_P_sp1')\n",
    "P = Metabolite('P', compartment='c')\n",
    "EX_P_sp1.add_metabolites({P:-1})\n",
    "EX_P_sp1.lower_bound = 0\n",
    "EX_P_sp1.upper_bound = 100\n",
    "Toy_Model_NE_1.add_reaction(EX_P_sp1)\n",
    "\n",
    "\n",
    "R_1_sp1 = Reaction('R_1_sp1')\n",
    "ADP = Metabolite('ADP', compartment='c')\n",
    "ATP = Metabolite('ATP', compartment='c')\n",
    "R_1_sp1.add_metabolites({ADP: -2, S: -1, P: 1, ATP: 2})\n",
    "R_1_sp1.lower_bound = 0\n",
    "R_1_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_1_sp1)\n",
    "\n",
    "\n",
    "R_2_sp1 = Reaction('R_2_sp1')\n",
    "R_2_sp1.add_metabolites({ADP: 1, P: -1, B: 1, ATP: -1})\n",
    "R_2_sp1.lower_bound = 0\n",
    "R_2_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_2_sp1)\n",
    "\n",
    "\n",
    "R_3_sp1 = Reaction('R_3_sp1')\n",
    "R_3_sp1.add_metabolites({ADP: 3, P: -1, A: 1, ATP: -3})\n",
    "R_3_sp1.lower_bound = 0\n",
    "R_3_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_3_sp1)\n",
    "\n",
    "\n",
    "\n",
    "R_4_sp1 = Reaction('R_4_sp1')\n",
    "R_4_sp1.add_metabolites({ADP:1 ,ATP: -1})\n",
    "R_4_sp1.lower_bound = 0\n",
    "R_4_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_4_sp1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OBJ_sp1 = Reaction(\"OBJ_sp1\")\n",
    "biomass_sp1 = Metabolite('biomass_sp1', compartment='c')\n",
    "OBJ_sp1.add_metabolites({ADP:5 ,ATP: -5,biomass_sp1:0.01,A:-3,B:-3})\n",
    "OBJ_sp1.lower_bound = 0\n",
    "OBJ_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(OBJ_sp1)\n",
    "\n",
    "Biomass_1 = Reaction(\"Biomass_1\")\n",
    "Biomass_1.add_metabolites({biomass_sp1:-1})\n",
    "Biomass_1.lower_bound = 0\n",
    "Biomass_1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(Biomass_1)\n",
    "\n",
    "Toy_Model_NE_1.objective='Biomass_1'\n",
    "Toy_Model_NE_1.Biomass_Ind=9\n",
    "\n",
    "\n",
    "### ADP Production From Catabolism ###\n",
    "\n",
    "Toy_Model_NE_2 = Model('Toy_2')\n",
    "\n",
    "### S_Uptake ###\n",
    "\n",
    "EX_S_sp2 = Reaction('EX_S_sp2')\n",
    "S = Metabolite('S', compartment='c')\n",
    "EX_S_sp2.add_metabolites({S: -1})\n",
    "EX_S_sp2.lower_bound = -10\n",
    "EX_S_sp2.upper_bound = 0\n",
    "Toy_Model_NE_2.add_reaction(EX_S_sp2)\n",
    "\n",
    "\n",
    "EX_A_sp2 = Reaction('EX_A_sp2')\n",
    "A = Metabolite('A', compartment='c')\n",
    "EX_A_sp2.add_metabolites({A: -1})\n",
    "EX_A_sp2.lower_bound = -100\n",
    "EX_A_sp2.upper_bound = 100\n",
    "Toy_Model_NE_2.add_reaction(EX_A_sp2)\n",
    "\n",
    "\n",
    "EX_B_sp2 = Reaction('EX_B_sp2')\n",
    "B = Metabolite('B', compartment='c')\n",
    "EX_B_sp2.add_metabolites({B: -1})\n",
    "EX_B_sp2.lower_bound = -100\n",
    "EX_B_sp2.upper_bound = 100\n",
    "Toy_Model_NE_2.add_reaction(EX_B_sp2)\n",
    "\n",
    "\n",
    "\n",
    "EX_P_sp2 = Reaction('EX_P_sp2')\n",
    "P = Metabolite('P', compartment='c')\n",
    "EX_P_sp2.add_metabolites({P:-1})\n",
    "EX_P_sp2.lower_bound = 0\n",
    "EX_P_sp2.upper_bound = 100\n",
    "Toy_Model_NE_2.add_reaction(EX_P_sp2)\n",
    "\n",
    "\n",
    "R_1_sp2 = Reaction('R_1_sp2')\n",
    "ADP = Metabolite('ADP', compartment='c')\n",
    "ATP = Metabolite('ATP', compartment='c')\n",
    "R_1_sp2.add_metabolites({ADP: -2, S: -1, P: 1, ATP: 2})\n",
    "R_1_sp2.lower_bound = 0\n",
    "R_1_sp2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(R_1_sp2)\n",
    "\n",
    "\n",
    "R_2_sp2 = Reaction('R_2_sp2')\n",
    "R_2_sp2.add_metabolites({ADP: 3, P: -1, B: 1, ATP: -3})\n",
    "R_2_sp2.lower_bound = 0\n",
    "R_2_sp2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(R_2_sp2)\n",
    "\n",
    "\n",
    "R_3_sp2 = Reaction('R_3_sp2')\n",
    "R_3_sp2.add_metabolites({ADP: 1, P: -1, A: 1, ATP: -1})\n",
    "R_3_sp2.lower_bound = 0\n",
    "R_3_sp2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(R_3_sp2)\n",
    "\n",
    "\n",
    "\n",
    "R_4_sp2 = Reaction('R_4_sp2')\n",
    "R_4_sp2.add_metabolites({ADP:1 ,ATP: -1})\n",
    "R_4_sp2.lower_bound = 0\n",
    "R_4_sp2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(R_4_sp2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OBJ_sp2 = Reaction(\"OBJ_sp2\")\n",
    "biomass_sp2 = Metabolite('biomass_sp2', compartment='c')\n",
    "OBJ_sp2.add_metabolites({ADP:5 ,ATP: -5,biomass_sp2:0.01,A:-3,B:-3})\n",
    "OBJ_sp2.lower_bound = 0\n",
    "OBJ_sp2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(OBJ_sp2)\n",
    "\n",
    "Biomass_2 = Reaction(\"Biomass_2\")\n",
    "Biomass_2.add_metabolites({biomass_sp2:-1})\n",
    "Biomass_2.lower_bound = 0\n",
    "Biomass_2.upper_bound = 1000\n",
    "Toy_Model_NE_2.add_reaction(Biomass_2)\n",
    "Toy_Model_NE_2.objective=\"Biomass_2\"\n",
    "Toy_Model_NE_2.Biomass_Ind=9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(Toy_Model_NE_1.optimize().fluxes)\n",
    "    print(Toy_Model_NE_1.exchanges)\n",
    "    print(Toy_Model_NE_2.optimize().fluxes)\n",
    "    print(Toy_Model_NE_2.exchanges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8a56cc54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_BATCHES=100\n",
    "BATCH_SIZE=8\n",
    "HIDDEN_SIZE=30\n",
    "PERCENTILE=70\n",
    "\n",
    "import os \n",
    "import datetime\n",
    "import numpy as np\n",
    "import cobra\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import pandas\n",
    "#import cplex\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple\n",
    "import ray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorboardX import SummaryWriter\n",
    "from heapq import heappop, heappush"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b44a7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Then we have to define a number of classes, objects, and functions that will be used during the simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "89f8f3ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Scaler=StandardScaler()\n",
    "CORES = multiprocessing.cpu_count()\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "Main_dir = os.path.dirname(\".\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class ProrityQueue:\n",
    "    \n",
    "    def __init__(self,N):\n",
    "        self.N=N\n",
    "        self.Elements=[]\n",
    "    \n",
    "    def enqueue_with_priority(self,Step):\n",
    "        Element = (-Step[0], random.random(),Step[1],Step[2])\n",
    "        heappush(self.Elements, Element)\n",
    "\n",
    "    def dequeue(self):\n",
    "        heappop(self.Elements)\n",
    "    \n",
    "    def balance(self):\n",
    "        while len(self.Elements)>=self.N:\n",
    "            self.dequeue()\n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcabd8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of DFBA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "269a9c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def dFBA(Models, Mapping_Dict: dict, Init_C:np.ndarray, Params:dict, t_span:list, dt=0.1)->list:\n",
    "    \"\"\"\n",
    "    This function calculates the concentration of each species\n",
    "    Models is a list of COBRA Model objects\n",
    "    Mapping_Dict is a dictionary of dictionaries\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    # Initializing the ODE Solver\n",
    "    ##############################################################\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    ##############################################################\n",
    "    # Solving the ODE\n",
    "    ##############################################################\n",
    "    for m in Models:\n",
    "        m.episode_reward=0\n",
    "        m.episode_steps=[]\n",
    "    \n",
    "    sol, t = odeFwdEuler(ODE_System, Init_C, dt,  Params,\n",
    "                         t_span, Models, Mapping_Dict)\n",
    "    \n",
    "    for m in Models:\n",
    "        m.Episode=Episode(reward=m.episode_reward, steps=m.episode_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return [m.Episode for m in Models]\n",
    "\n",
    "\n",
    "def ODE_System(C, t, Models, Mapping_Dict, Params, dt):\n",
    "    \"\"\"\n",
    "    This function calculates the differential equations for the system\n",
    "    Models is a list of COBRA Model objects\n",
    "    NOTE: this implementation of DFBA is compatible with RL framework\n",
    "    Given a policy it will genrate episodes. Policies can be either deterministic or stochastic\n",
    "    Differential Equations Are Formatted as follows:\n",
    "    [0]-Models[1]\n",
    "    [1]-Models[2]\n",
    "    []-...\n",
    "    [n-1]-Models[n]\n",
    "    [n]-Exc[1]\n",
    "    [n+1]-Exc[2]\n",
    "    []-...\n",
    "    [n+m-1]-Exc[m]\n",
    "    [n+m]-Starch\n",
    "    \"\"\"\n",
    "    C[C < 0] = 0\n",
    "    dCdt = np.zeros(C.shape)\n",
    "    Sols = list([0 for i in range(Models.__len__())])\n",
    "    for i,M in enumerate(Models):\n",
    "        \n",
    "        if random.random()<M.epsilon:\n",
    "\n",
    "            M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]*(1-M.epsilon)+np.random.uniform(low=-0.2, high=0.2,size=len(M.actions))*M.epsilon\n",
    "        \n",
    "        else:\n",
    "\n",
    "            M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]\n",
    "        \n",
    "        for index,item in enumerate(Mapping_Dict[\"Ex_sp\"]):\n",
    "            if Mapping_Dict['Mapping_Matrix'][index,i]!=-1:\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].upper_bound=20\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].lower_bound=-General_Uptake_Kinetics(C[index+len(Models)])\n",
    "                \n",
    "            \n",
    "        for index,flux in enumerate(M.actions):\n",
    "            M.a[index]=Flux_Clipper(M.reactions[M.actions[index]].lower_bound,M.a[index],M.reactions[M.actions[index]].upper_bound)\n",
    "            M.reactions[M.actions[index]].lower_bound=M.a[index]\n",
    "            # M.reactions[M.actions[index]].upper_bound=M.a[index]\n",
    "\n",
    "        Sols[i] = Models[i].optimize()\n",
    "\n",
    "        if Sols[i].status == 'infeasible':\n",
    "            Models[i].reward= -10\n",
    "            dCdt[i] = 0\n",
    "\n",
    "        else:\n",
    "            dCdt[i] += Sols[i].objective_value*C[i]\n",
    "            Models[i].reward =Sols[i].objective_value\n",
    "\n",
    "\n",
    "\n",
    "    ### Writing the balance equations\n",
    "\n",
    "    for i in range(Mapping_Dict[\"Mapping_Matrix\"].shape[0]):\n",
    "        for j in range(len(Models)):\n",
    "            if Mapping_Dict[\"Mapping_Matrix\"][i, j] != -1:\n",
    "                if Sols[j].status == 'infeasible':\n",
    "                    dCdt[i] = 0\n",
    "                else:\n",
    "                    dCdt[i+len(Models)] += Sols[j].fluxes.iloc[Mapping_Dict[\"Mapping_Matrix\"]\n",
    "                                                                    [i, j]]*C[j]\n",
    "\n",
    "\n",
    "    for m in Models:\n",
    "        m.episode_reward += m.reward\n",
    "        m.episode_steps.append(EpisodeStep(observation=C[m.observables], action=m.a))\n",
    "    \n",
    "    dCdt += np.array(Params[\"Dilution_Rate\"])*(Params[\"Inlet_C\"]-C)\n",
    "    \n",
    "    return dCdt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb1220",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now we need some utility functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "69f08830",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Build_Mapping_Matrix(Models):\n",
    "    \"\"\"\n",
    "    Given a list of COBRA model objects, this function will build a mapping matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Ex_sp = []\n",
    "    Temp_Map={}\n",
    "    for model in Models:\n",
    "        \n",
    "        \n",
    "        if not hasattr(model,\"Biomass_Ind\"):\n",
    "            raise Exception(\"Models must have 'Biomass_Ind' attribute in order for the DFBA to work properly!\")\n",
    "        \n",
    "        \n",
    "        for Ex_rxn in model.exchanges :\n",
    "            if Ex_rxn!=model.reactions[model.Biomass_Ind]:\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id not in Ex_sp:\n",
    "                    Ex_sp.append(list(Ex_rxn.metabolites.keys())[0].id)\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id in Temp_Map.keys():\n",
    "                   Temp_Map[list(Ex_rxn.metabolites.keys())[0].id][model]=Ex_rxn\n",
    "                else:\n",
    "                     Temp_Map[list(Ex_rxn.metabolites.keys())[0].id]={model:Ex_rxn}\n",
    "\n",
    "    Mapping_Matrix = np.zeros((len(Ex_sp), len(Models)), dtype=int)\n",
    "    for i, id in enumerate(Ex_sp):\n",
    "        for j, model in enumerate(Models):\n",
    "            if model in Temp_Map[id].keys():\n",
    "                Mapping_Matrix[i, j] = model.reactions.index(Temp_Map[id][model].id)\n",
    "            else:\n",
    "                Mapping_Matrix[i, j] = -1\n",
    "    return {\"Ex_sp\": Ex_sp, \"Mapping_Matrix\": Mapping_Matrix}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def General_Uptake_Kinetics(Compound: float, Model=\"\"):\n",
    "    \"\"\"\n",
    "    This function calculates the rate of uptake of a compound in the reactor\n",
    "    ###It is just a simple imaginary model: Replace it with better model if necessary###\n",
    "    Compound Unit: mmol\n",
    "\n",
    "    \"\"\"\n",
    "    return 30*(Compound/(Compound+20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def odeFwdEuler(ODE_Function, ICs, dt, Params, t_span, Models, Mapping_Dict):\n",
    "    Integrator_Counter = 0\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    sol = np.zeros((len(t), len(ICs)))\n",
    "    sol[0] = ICs\n",
    "    for i in range(1, len(t)):\n",
    "        sol[i] = sol[i-1] + \\\n",
    "            ODE_Function(sol[i-1], t[i-1], Models, Mapping_Dict,\n",
    "                         Params, dt)*dt\n",
    "        Integrator_Counter += 1\n",
    "    return sol, t\n",
    "\n",
    "\n",
    "def Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict, Batch_Size=10,t_span=[0, 100], dt=0.1):\n",
    "\n",
    "\n",
    "    Init_C[list(Params[\"Env_States\"])] = [random.uniform(Range[0], Range[1]) for Range in Params[\"Env_States_Initial_Ranges\"]]\n",
    "    \n",
    "\n",
    "    \n",
    "    Batch_Episodes=[]\n",
    "    for BATCH in range(Batch_Size):\n",
    "        Batch_Episodes.append(dFBA.remote(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "        # Batch_Episodes.append(dFBA(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "\n",
    "    return(ray.get(Batch_Episodes))    \n",
    "\n",
    "    # return(Batch_Episodes)    \n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.FloatTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n",
    "\n",
    "def Flux_Clipper(Min,Number,Max):\n",
    "    return(min(max(Min,Number),Max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccf07c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We now define the high level main function that controls every part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4a19e38c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def main(Models: list = [Toy_Model_NE_1.copy(), Toy_Model_NE_2.copy()], max_time: int = 100, Dil_Rate: float = 0.000000001, alpha: float = 0.01, Starting_Q: str = \"FBA\")->None:\n",
    "    \"\"\"\n",
    "    This is the main function for running dFBA.\n",
    "    The main requrement for working properly is\n",
    "    that the models use the same notation for the\n",
    "    same reactions.\n",
    "\n",
    "    Starting_Policy:\n",
    "\n",
    "    Defult --> Random: Initial Policy will be a random policy for all agents.\n",
    "    Otherwise --> a list of policies, pickle file addresses, for each agent.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Adding Agents info ###-----------------------------------------------------\n",
    "\n",
    "    # State dimensions in this RLDFBA variant include: [Agent1,...,Agentn, glucose,starch]\n",
    "    for i in range(len(Models)):\n",
    "        if not hasattr(Models[i], \"_name\"):\n",
    "            Models[i].NAME = \"Agent_\" + str(i)\n",
    "            print(f\"Agent {i} has been given a defult name\")\n",
    "        Models[i].solver.objective.name = \"_pfba_objective\"\n",
    "    # -------------------------------------------------------------------------------\n",
    "\n",
    "    # Mapping internal reactions to external reactions, and operational parameter\n",
    "    # setup ###-------------------------------------------------------------------\n",
    "\n",
    "    # For more information about the structure of the ODEs,see ODE_System function\n",
    "    # or the documentation.\n",
    "\n",
    "    Mapping_Dict = Build_Mapping_Matrix(Models)\n",
    "    Init_C = np.ones((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "    Inlet_C = np.zeros((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "\n",
    "    #Parameters that are use inside DFBA\n",
    "\n",
    "    Params = {\n",
    "        \"Dilution_Rate\": Dil_Rate,\n",
    "        \"Inlet_C\": Inlet_C,\n",
    "        \"Agents_Index\": [i for i in range(len(Models))],\n",
    "    }\n",
    "\n",
    "    #Define Agent attributes\n",
    "    Obs=[i for i in range(len(Models))]\n",
    "    Obs.extend([Mapping_Dict[\"Ex_sp\"].index(sp)+len(Models) for sp in Mapping_Dict[\"Ex_sp\"] if sp!='P' ])\n",
    "    for ind,m in enumerate(Models):\n",
    "        m.observables=Obs\n",
    "        m.actions=(Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"A\"),ind],Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"B\"),ind])\n",
    "        m.Policy=Net(len(m.observables), HIDDEN_SIZE, len(m.actions))\n",
    "        m.optimizer=optim.SGD(params=m.Policy.parameters(), lr=0.01)\n",
    "        m.Net_Obj=nn.MSELoss()\n",
    "        m.epsilon=0.05\n",
    "        \n",
    "    ### I Assume that the environment states are all observable. Env states will be stochastic\n",
    "    Params[\"Env_States\"]=Models[0].observables\n",
    "    Params[\"Env_States_Initial_Ranges\"]=[[0.1,0.1+0.00000001],[0.1,0.1+0.00000001],[100,100+0.00001],[0,0+0.00000000001],[0,0+0.00000000001]]\n",
    "    for i in range(len(Models)):\n",
    "        Init_C[i] = 0.001\n",
    "        #Models[i].solver = \"cplex\"\n",
    "    writer = SummaryWriter(comment=\"-DeepRLDFBA_NECOM\")\n",
    "    Outer_Counter = 0\n",
    "\n",
    "\n",
    "    for c in range(NUMBER_OF_BATCHES):\n",
    "        for m in Models:\n",
    "            m.epsilon=0.01+0.99/(np.exp(c/20))\n",
    "        Batch_Out=Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict,Batch_Size=BATCH_SIZE)\n",
    "        Batch_Out=list(map(list, zip(*Batch_Out)))\n",
    "        for index,Model in enumerate(Models):\n",
    "            obs_v, acts_v, reward_b, reward_m=filter_batch(Batch_Out[index], PERCENTILE)\n",
    "            Model.optimizer.zero_grad()\n",
    "            action_scores_v = Model.Policy(obs_v)\n",
    "            loss_v = Model.Net_Obj(action_scores_v, acts_v)\n",
    "            loss_v.backward()\n",
    "            Model.optimizer.step()\n",
    "            print(f\"{Model.NAME}\")\n",
    "            print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (c, loss_v.item(), reward_m, reward_b))\n",
    "\n",
    "            writer.add_scalar(f\"{Model.NAME} reward_mean\", reward_m, c)\n",
    "    \n",
    "    Time=datetime.datetime.now().strftime(\"%d_%m_%Y.%H_%M_%S\")\n",
    "    Results_Dir=os.path.join(Main_dir,\"Outputs\",str(Time))\n",
    "    os.mkdir(Results_Dir)\n",
    "    with open(os.path.join(Results_Dir,\"Models.pkl\"),'wb') as f:\n",
    "        pickle.dump(Models,f)\n",
    "    return Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a94ccd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finally it's time to run!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "94a98600",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 has been given a defult name\n",
      "Agent 1 has been given a defult name\n",
      "Agent_0\n",
      "0: loss=0.029, reward_mean=-4767.9, reward_bound=-4679.4\n",
      "Agent_1\n",
      "0: loss=0.026, reward_mean=-4859.1, reward_bound=-4785.4\n",
      "Agent_0\n",
      "1: loss=0.025, reward_mean=-4626.6, reward_bound=-4626.4\n",
      "Agent_1\n",
      "1: loss=0.021, reward_mean=-5052.9, reward_bound=-4976.4\n",
      "Agent_0\n",
      "2: loss=0.022, reward_mean=-4559.1, reward_bound=-4508.3\n",
      "Agent_1\n",
      "2: loss=0.020, reward_mean=-5171.6, reward_bound=-5059.4\n",
      "Agent_0\n",
      "3: loss=0.019, reward_mean=-4520.3, reward_bound=-4443.3\n",
      "Agent_1\n",
      "3: loss=0.017, reward_mean=-5359.1, reward_bound=-5328.4\n",
      "Agent_0\n",
      "4: loss=0.016, reward_mean=-4382.8, reward_bound=-4316.3\n",
      "Agent_1\n",
      "4: loss=0.015, reward_mean=-5429.1, reward_bound=-5334.4\n",
      "Agent_0\n",
      "5: loss=0.014, reward_mean=-4280.3, reward_bound=-4201.3\n",
      "Agent_1\n",
      "5: loss=0.013, reward_mean=-5531.6, reward_bound=-5461.4\n",
      "Agent_0\n",
      "6: loss=0.012, reward_mean=-4051.6, reward_bound=-3969.3\n",
      "Agent_1\n",
      "6: loss=0.011, reward_mean=-5725.4, reward_bound=-5686.4\n",
      "Agent_0\n",
      "7: loss=0.010, reward_mean=-4096.6, reward_bound=-3965.3\n",
      "Agent_1\n",
      "7: loss=0.010, reward_mean=-5831.6, reward_bound=-5788.4\n",
      "Agent_0\n",
      "8: loss=0.009, reward_mean=-3912.8, reward_bound=-3725.3\n",
      "Agent_1\n",
      "8: loss=0.008, reward_mean=-5992.9, reward_bound=-5885.4\n",
      "Agent_0\n",
      "9: loss=0.007, reward_mean=-3827.8, reward_bound=-3784.3\n",
      "Agent_1\n",
      "9: loss=0.007, reward_mean=-6000.4, reward_bound=-5925.4\n",
      "Agent_0\n",
      "10: loss=0.006, reward_mean=-3400.3, reward_bound=-3272.3\n",
      "Agent_1\n",
      "10: loss=0.006, reward_mean=-6152.9, reward_bound=-6119.4\n",
      "Agent_0\n",
      "11: loss=0.005, reward_mean=-3249.1, reward_bound=-3085.3\n",
      "Agent_1\n",
      "11: loss=0.005, reward_mean=-6211.6, reward_bound=-6156.4\n",
      "Agent_0\n",
      "12: loss=0.005, reward_mean=-3035.3, reward_bound=-2832.3\n",
      "Agent_1\n",
      "12: loss=0.004, reward_mean=-6272.9, reward_bound=-6256.4\n",
      "Agent_0\n",
      "13: loss=0.004, reward_mean=-2842.8, reward_bound=-2870.3\n",
      "Agent_1\n",
      "13: loss=0.004, reward_mean=-6340.4, reward_bound=-6306.4\n",
      "Agent_0\n",
      "14: loss=0.004, reward_mean=-1930.3, reward_bound=-1439.3\n",
      "Agent_1\n",
      "14: loss=0.003, reward_mean=-6444.1, reward_bound=-6396.4\n",
      "Agent_0\n",
      "15: loss=0.003, reward_mean=-2022.8, reward_bound=-1823.3\n",
      "Agent_1\n",
      "15: loss=0.003, reward_mean=-6485.4, reward_bound=-6455.4\n",
      "Agent_0\n",
      "16: loss=0.003, reward_mean=-1354.1, reward_bound=-1075.3\n",
      "Agent_1\n",
      "16: loss=0.002, reward_mean=-6517.9, reward_bound=-6516.4\n",
      "Agent_0\n",
      "17: loss=0.002, reward_mean=-912.8, reward_bound=-897.3\n",
      "Agent_1\n",
      "17: loss=0.002, reward_mean=-6586.6, reward_bound=-6559.4\n",
      "Agent_0\n",
      "18: loss=0.002, reward_mean=-841.5, reward_bound=-807.3\n",
      "Agent_1\n",
      "18: loss=0.002, reward_mean=-6616.6, reward_bound=-6597.4\n",
      "Agent_0\n",
      "19: loss=0.002, reward_mean=-725.3, reward_bound=-676.3\n",
      "Agent_1\n",
      "19: loss=0.002, reward_mean=-6650.4, reward_bound=-6645.4\n",
      "Agent_0\n",
      "20: loss=0.002, reward_mean=-554.0, reward_bound=-498.3\n",
      "Agent_1\n",
      "20: loss=0.001, reward_mean=-6674.1, reward_bound=-6666.4\n",
      "Agent_0\n",
      "21: loss=0.001, reward_mean=-555.3, reward_bound=-525.3\n",
      "Agent_1\n",
      "21: loss=0.001, reward_mean=-6705.4, reward_bound=-6696.4\n",
      "Agent_0\n",
      "22: loss=0.001, reward_mean=-401.5, reward_bound=-376.3\n",
      "Agent_1\n",
      "22: loss=0.001, reward_mean=-6741.6, reward_bound=-6717.4\n",
      "Agent_0\n",
      "23: loss=0.001, reward_mean=-341.5, reward_bound=-298.3\n",
      "Agent_1\n",
      "23: loss=0.001, reward_mean=-6756.6, reward_bound=-6747.4\n",
      "Agent_0\n",
      "24: loss=0.001, reward_mean=-199.0, reward_bound=-156.3\n",
      "Agent_1\n",
      "24: loss=0.001, reward_mean=-6772.9, reward_bound=-6757.4\n",
      "Agent_0\n",
      "25: loss=0.001, reward_mean=-144.0, reward_bound=-118.3\n",
      "Agent_1\n",
      "25: loss=0.001, reward_mean=-6794.1, reward_bound=-6786.4\n",
      "Agent_0\n",
      "26: loss=0.001, reward_mean=-110.3, reward_bound=-68.3\n",
      "Agent_1\n",
      "26: loss=0.001, reward_mean=-6807.9, reward_bound=-6805.4\n",
      "Agent_0\n",
      "27: loss=0.001, reward_mean=-27.8, reward_bound=-15.3\n",
      "Agent_1\n",
      "27: loss=0.000, reward_mean=-6822.9, reward_bound=-6815.4\n",
      "Agent_0\n",
      "28: loss=0.001, reward_mean=-0.3, reward_bound=4.7\n",
      "Agent_1\n",
      "28: loss=0.000, reward_mean=-6834.1, reward_bound=-6825.4\n",
      "Agent_0\n",
      "29: loss=0.001, reward_mean=4.7, reward_bound=4.7\n",
      "Agent_1\n",
      "29: loss=0.000, reward_mean=-6850.4, reward_bound=-6845.4\n",
      "Agent_0\n",
      "30: loss=0.001, reward_mean=4.7, reward_bound=4.7\n",
      "Agent_1\n",
      "30: loss=0.000, reward_mean=-6857.9, reward_bound=-6855.4\n",
      "Agent_0\n",
      "31: loss=0.001, reward_mean=4.7, reward_bound=4.7\n",
      "Agent_1\n",
      "31: loss=0.000, reward_mean=-6867.9, reward_bound=-6865.4\n",
      "Agent_0\n",
      "32: loss=0.001, reward_mean=4.7, reward_bound=4.7\n",
      "Agent_1\n",
      "32: loss=0.000, reward_mean=-6872.9, reward_bound=-6865.4\n",
      "Agent_0\n",
      "33: loss=0.001, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "33: loss=0.000, reward_mean=-6880.4, reward_bound=-6875.4\n",
      "Agent_0\n",
      "34: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "34: loss=0.000, reward_mean=-6891.6, reward_bound=-6885.4\n",
      "Agent_0\n",
      "35: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "35: loss=0.000, reward_mean=-6892.9, reward_bound=-6886.4\n",
      "Agent_0\n",
      "36: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "36: loss=0.000, reward_mean=-6900.4, reward_bound=-6895.4\n",
      "Agent_0\n",
      "37: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "37: loss=0.000, reward_mean=-6911.6, reward_bound=-6905.4\n",
      "Agent_0\n",
      "38: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "38: loss=0.000, reward_mean=-6912.9, reward_bound=-6905.4\n",
      "Agent_0\n",
      "39: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "39: loss=0.000, reward_mean=-6919.1, reward_bound=-6915.4\n",
      "Agent_0\n",
      "40: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "40: loss=0.000, reward_mean=-6920.4, reward_bound=-6915.4\n",
      "Agent_0\n",
      "41: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "41: loss=0.000, reward_mean=-6925.4, reward_bound=-6925.4\n",
      "Agent_0\n",
      "42: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "42: loss=0.000, reward_mean=-6930.4, reward_bound=-6925.4\n",
      "Agent_0\n",
      "43: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "43: loss=0.000, reward_mean=-6936.6, reward_bound=-6935.4\n",
      "Agent_0\n",
      "44: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "44: loss=0.000, reward_mean=-6942.9, reward_bound=-6936.4\n",
      "Agent_0\n",
      "45: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "45: loss=0.000, reward_mean=-6944.1, reward_bound=-6945.4\n",
      "Agent_0\n",
      "46: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "46: loss=0.000, reward_mean=-6946.6, reward_bound=-6945.4\n",
      "Agent_0\n",
      "47: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "47: loss=0.000, reward_mean=-6949.1, reward_bound=-6945.4\n",
      "Agent_0\n",
      "48: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "48: loss=0.000, reward_mean=-6950.4, reward_bound=-6945.4\n",
      "Agent_0\n",
      "49: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "49: loss=0.000, reward_mean=-6955.4, reward_bound=-6955.4\n",
      "Agent_0\n",
      "50: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "50: loss=0.000, reward_mean=-6955.4, reward_bound=-6955.4\n",
      "Agent_0\n",
      "51: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "51: loss=0.000, reward_mean=-6956.6, reward_bound=-6955.4\n",
      "Agent_0\n",
      "52: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "52: loss=0.000, reward_mean=-6961.6, reward_bound=-6956.4\n",
      "Agent_0\n",
      "53: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "53: loss=0.000, reward_mean=-6965.4, reward_bound=-6965.4\n",
      "Agent_0\n",
      "54: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "54: loss=0.000, reward_mean=-6967.9, reward_bound=-6965.4\n",
      "Agent_0\n",
      "55: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "55: loss=0.000, reward_mean=-6965.4, reward_bound=-6965.4\n",
      "Agent_0\n",
      "56: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "56: loss=0.000, reward_mean=-6969.1, reward_bound=-6965.4\n",
      "Agent_0\n",
      "57: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "57: loss=0.000, reward_mean=-6966.6, reward_bound=-6965.4\n",
      "Agent_0\n",
      "58: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "58: loss=0.000, reward_mean=-6975.4, reward_bound=-6975.4\n",
      "Agent_0\n",
      "59: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "59: loss=0.000, reward_mean=-6976.6, reward_bound=-6975.4\n",
      "Agent_0\n",
      "60: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "60: loss=0.000, reward_mean=-6976.6, reward_bound=-6975.4\n",
      "Agent_0\n",
      "61: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "61: loss=0.000, reward_mean=-6980.4, reward_bound=-6975.4\n",
      "Agent_0\n",
      "62: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "62: loss=0.000, reward_mean=-6977.9, reward_bound=-6975.4\n",
      "Agent_0\n",
      "63: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "63: loss=0.000, reward_mean=-6977.9, reward_bound=-6975.4\n",
      "Agent_0\n",
      "64: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "64: loss=0.000, reward_mean=-6975.4, reward_bound=-6975.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent_0\n",
      "65: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "65: loss=0.000, reward_mean=-6982.9, reward_bound=-6985.4\n",
      "Agent_0\n",
      "66: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "66: loss=0.000, reward_mean=-6980.4, reward_bound=-6975.4\n",
      "Agent_0\n",
      "67: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "67: loss=0.000, reward_mean=-6982.9, reward_bound=-6985.4\n",
      "Agent_0\n",
      "68: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "68: loss=0.000, reward_mean=-6987.9, reward_bound=-6985.4\n",
      "Agent_0\n",
      "69: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "69: loss=0.000, reward_mean=-6990.4, reward_bound=-6985.4\n",
      "Agent_0\n",
      "70: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "70: loss=0.000, reward_mean=-6989.1, reward_bound=-6985.4\n",
      "Agent_0\n",
      "71: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "71: loss=0.000, reward_mean=-6990.4, reward_bound=-6985.4\n",
      "Agent_0\n",
      "72: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "72: loss=0.000, reward_mean=-6989.1, reward_bound=-6985.4\n",
      "Agent_0\n",
      "73: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "73: loss=0.000, reward_mean=-6990.4, reward_bound=-6985.4\n",
      "Agent_0\n",
      "74: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "74: loss=0.000, reward_mean=-6990.4, reward_bound=-6985.4\n",
      "Agent_0\n",
      "75: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "75: loss=0.000, reward_mean=-6994.1, reward_bound=-6995.4\n",
      "Agent_0\n",
      "76: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "76: loss=0.000, reward_mean=-6996.6, reward_bound=-6995.4\n",
      "Agent_0\n",
      "77: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "77: loss=0.000, reward_mean=-6992.9, reward_bound=-6995.4\n",
      "Agent_0\n",
      "78: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "78: loss=0.000, reward_mean=-6995.4, reward_bound=-6995.4\n",
      "Agent_0\n",
      "79: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "79: loss=0.000, reward_mean=-6997.9, reward_bound=-6995.4\n",
      "Agent_0\n",
      "80: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "80: loss=0.000, reward_mean=-6999.1, reward_bound=-6995.4\n",
      "Agent_0\n",
      "81: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "81: loss=0.000, reward_mean=-7000.4, reward_bound=-6995.4\n",
      "Agent_0\n",
      "82: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "82: loss=0.000, reward_mean=-7000.4, reward_bound=-6995.4\n",
      "Agent_0\n",
      "83: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "83: loss=0.000, reward_mean=-6997.9, reward_bound=-6995.4\n",
      "Agent_0\n",
      "84: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "84: loss=0.000, reward_mean=-7002.9, reward_bound=-7005.4\n",
      "Agent_0\n",
      "85: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "85: loss=0.000, reward_mean=-7001.6, reward_bound=-6996.4\n",
      "Agent_0\n",
      "86: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "86: loss=0.000, reward_mean=-7004.1, reward_bound=-7005.4\n",
      "Agent_0\n",
      "87: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "87: loss=0.000, reward_mean=-7004.1, reward_bound=-7005.4\n",
      "Agent_0\n",
      "88: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "88: loss=0.000, reward_mean=-7006.6, reward_bound=-7005.4\n",
      "Agent_0\n",
      "89: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "89: loss=0.000, reward_mean=-7006.6, reward_bound=-7005.4\n",
      "Agent_0\n",
      "90: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "90: loss=0.000, reward_mean=-7005.4, reward_bound=-7005.4\n",
      "Agent_0\n",
      "91: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "91: loss=0.000, reward_mean=-7006.6, reward_bound=-7005.4\n",
      "Agent_0\n",
      "92: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "92: loss=0.000, reward_mean=-7006.6, reward_bound=-7005.4\n",
      "Agent_0\n",
      "93: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "93: loss=0.000, reward_mean=-7007.9, reward_bound=-7005.4\n",
      "Agent_0\n",
      "94: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "94: loss=0.000, reward_mean=-7007.9, reward_bound=-7005.4\n",
      "Agent_0\n",
      "95: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "95: loss=0.000, reward_mean=-7011.6, reward_bound=-7006.4\n",
      "Agent_0\n",
      "96: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "96: loss=0.000, reward_mean=-7006.6, reward_bound=-7005.4\n",
      "Agent_0\n",
      "97: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "97: loss=0.000, reward_mean=-7009.1, reward_bound=-7005.4\n",
      "Agent_0\n",
      "98: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "98: loss=0.000, reward_mean=-7007.9, reward_bound=-7005.4\n",
      "Agent_0\n",
      "99: loss=0.000, reward_mean=4.8, reward_bound=4.8\n",
      "Agent_1\n",
      "99: loss=0.000, reward_mean=-7009.1, reward_bound=-7005.4\n"
     ]
    }
   ],
   "source": [
    "Models=main([Toy_Model_NE_1.copy(),Toy_Model_NE_2.copy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16c5d2",
   "metadata": {},
   "source": [
    "## Step n: Plotting the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d87af8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observation space of each agent **in this particular example** is similar and contains 5 different states. Our goal is to create a meshgrid dataframe that includes these states and the actions taken as columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82225986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0d290",
   "metadata": {},
   "source": [
    "One way is to make a mesh grid which might be inefficient, probably is! So I won't complete it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c94f9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ag1=np.linspace(0,10,10)\n",
    "Ag2=np.linspace(0,10,10)\n",
    "S=np.linspace(0,100,10)\n",
    "A=np.linspace(0,10,10)\n",
    "B=np.linspace(0,10,10)\n",
    "Meshgrid=np.meshgrid(Ag1,Ag2,S,A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d110434",
   "metadata": {},
   "source": [
    "A much faster way is to generate a bunch of random points in the desired mesh and input them to the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3ee0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Space_Size=10000\n",
    "Space=np.random.uniform(low=[0,0,0,0,0],high=[10,10,100,10,10],size=(Space_Size,5))\n",
    "Actions_1=Models[0].Policy(torch.FloatTensor(Space))\n",
    "Actions_2=Models[1].Policy(torch.FloatTensor(Space))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7dbd7c00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DF_1=pd.DataFrame(np.hstack((Space,Actions_1.detach().numpy())),columns=[\"Agent1\",\"Agent2\",\"S\",\"A\",\"B\",\"A_Export\",\"B_Export\"])\n",
    "\n",
    "DF_2=pd.DataFrame(np.hstack((Space,Actions_2.detach().numpy())),columns=[\"Agent1\",\"Agent2\",\"S\",\"A\",\"B\",\"A_Export\",\"B_Export\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ac32d63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7139"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(DF_2[\"A_Export\"]<0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49492ba7",
   "metadata": {},
   "source": [
    "## Run a simulation with the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e7f73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_t(Models: list = [Toy_Model_NE_1.copy(), Toy_Model_NE_2.copy()], max_time: int = 100, Dil_Rate: float = 0.000000001, alpha: float = 0.01, Starting_Q: str = \"FBA\"):\n",
    "    \"\"\"\n",
    "    This is the main function for running dFBA.\n",
    "    The main requrement for working properly is\n",
    "    that the models use the same notation for the\n",
    "    same reactions.\n",
    "\n",
    "    Starting_Policy:\n",
    "\n",
    "    Defult --> Random: Initial Policy will be a random policy for all agents.\n",
    "    Otherwise --> a list of policies, pickle file addresses, for each agent.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Mapping_Dict = Build_Mapping_Matrix(Models)\n",
    "    Init_C = np.ones((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "    Inlet_C = np.zeros((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "\n",
    "    #Parameters that are use inside DFBA\n",
    "\n",
    "    Params = {\n",
    "        \"Dilution_Rate\": Dil_Rate,\n",
    "        \"Inlet_C\": Inlet_C,\n",
    "        \"Agents_Index\": [i for i in range(len(Models))],\n",
    "    }\n",
    "\n",
    "    #Define Agent attributes\n",
    "\n",
    "    ### I Assume that the environment states are all observable. Env states will be stochastic\n",
    "    Params[\"Env_States\"]=Models[0].observables\n",
    "    Params[\"Env_States_Initial_Ranges\"]=[[0.1,0.100001],[0.1,0.100001],[50,50.001],[0.01,0.01+0.000001],[0.01,0.01+0.00001]]\n",
    "\n",
    "    Sol,t=Generate_Batch_t(dFBA, Params, Init_C, Models, Mapping_Dict)\n",
    "    return Sol,t\n",
    "    \n",
    "\n",
    "\n",
    "def dFBA_t(Models, Mapping_Dict, Init_C, Params, t_span, dt=0.1):\n",
    "    \"\"\"\n",
    "    This function calculates the concentration of each species\n",
    "    Models is a list of COBRA Model objects\n",
    "    Mapping_Dict is a dictionary of dictionaries\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    # Initializing the ODE Solver\n",
    "    ##############################################################\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    ##############################################################\n",
    "    # Solving the ODE\n",
    "    ##############################################################\n",
    "\n",
    "    \n",
    "    sol, t = odeFwdEuler(ODE_System_t, Init_C, dt,  Params,\n",
    "                         t_span, Models, Mapping_Dict)\n",
    "    \n",
    "    return sol,t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ODE_System_t(C, t, Models, Mapping_Dict, Params, dt):\n",
    "    \"\"\"\n",
    "    This function calculates the differential equations for the system\n",
    "    Models is a list of COBRA Model objects\n",
    "    NOTE: this implementation of DFBA is compatible with RL framework\n",
    "    Given a policy it will genrate episodes. Policies can be either deterministic or stochastic\n",
    "    Differential Equations Are Formatted as follows:\n",
    "    [0]-Models[1]\n",
    "    [1]-Models[2]\n",
    "    []-...\n",
    "    [n-1]-Models[n]\n",
    "    [n]-Exc[1]\n",
    "    [n+1]-Exc[2]\n",
    "    []-...\n",
    "    [n+m-1]-Exc[m]\n",
    "    [n+m]-Starch\n",
    "    \"\"\"\n",
    "    C[C < 0] = 0\n",
    "    dCdt = np.zeros(C.shape)\n",
    "    Sols = list([0 for i in range(Models.__len__())])\n",
    "    for i,M in enumerate(Models):\n",
    "        \n",
    "\n",
    "\n",
    "        M.a=M.Policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]\n",
    "        \n",
    "        for index,item in enumerate(Mapping_Dict[\"Ex_sp\"]):\n",
    "            if Mapping_Dict['Mapping_Matrix'][index,i]!=-1:\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].upper_bound=20\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].lower_bound=-General_Uptake_Kinetics(C[index+len(Models)])\n",
    "                \n",
    "            \n",
    "        for index,flux in enumerate(M.actions):\n",
    "            M.a[index]=Flux_Clipper(M.reactions[flux].lower_bound,M.a[index],M.reactions[flux].upper_bound)\n",
    "            M.reactions[flux].lower_bound=M.a[index]\n",
    "            M.reactions[flux].upper_bound=M.a[index]\n",
    "\n",
    "        Sols[i] = Models[i].optimize()\n",
    "\n",
    "        if Sols[i].status == 'infeasible':\n",
    "            dCdt[i] = 0\n",
    "\n",
    "        else:\n",
    "            dCdt[i] += Sols[i].objective_value*C[i]\n",
    "            Models[i].reward =Sols[i].objective_value\n",
    "\n",
    "\n",
    "\n",
    "    ### Writing the balance equations\n",
    "\n",
    "    for i in range(Mapping_Dict[\"Mapping_Matrix\"].shape[0]):\n",
    "        for j in range(len(Models)):\n",
    "            if Mapping_Dict[\"Mapping_Matrix\"][i, j] != -1:\n",
    "                if Sols[j].status == 'infeasible':\n",
    "                    dCdt[i] = 0\n",
    "                else:\n",
    "                    dCdt[i+len(Models)] += Sols[j].fluxes.iloc[Mapping_Dict[\"Mapping_Matrix\"]\n",
    "                                                                    [i, j]]*C[j]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    dCdt += np.array(Params[\"Dilution_Rate\"])*(Params[\"Inlet_C\"]-C)\n",
    "    \n",
    "    return dCdt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Generate_Batch_t(dFBA, Params, Init_C, Models, Mapping_Dict,t_span=[0, 100], dt=1):\n",
    "\n",
    "\n",
    "    Init_C[list(Params[\"Env_States\"])] = [random.uniform(Range[0], Range[1]) for Range in Params[\"Env_States_Initial_Ranges\"]]\n",
    "    \n",
    "\n",
    "    Sol,t=dFBA_t(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt)\n",
    "        # Batch_Episodes.append(dFBA(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt))\n",
    "\n",
    "    return Sol,t  \n",
    "\n",
    "\n",
    "Solution=main_t(Models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "db7c9200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x327558f10>,\n",
       " <matplotlib.lines.Line2D at 0x3275587f0>,\n",
       " <matplotlib.lines.Line2D at 0x327558c70>,\n",
       " <matplotlib.lines.Line2D at 0x32772a6d0>,\n",
       " <matplotlib.lines.Line2D at 0x351aa4790>,\n",
       " <matplotlib.lines.Line2D at 0x351aa49a0>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Solution[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7780ee53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000760e-01, 1.00000157e-01, 5.00001881e+01, 1.00009903e-02,\n",
       "        1.00072603e-02, 1.00000000e+00],\n",
       "       [1.00000760e-01, 1.00000157e-01, 5.00001880e+01, 1.00009903e-02,\n",
       "        1.14405691e-02, 3.00001519e+00],\n",
       "       [1.00000760e-01, 1.00000157e-01, 5.00001880e+01, 1.00009903e-02,\n",
       "        1.28738780e-02, 5.00003038e+00],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001879e+01, 1.00009903e-02,\n",
       "        1.43071869e-02, 7.00004557e+00],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001879e+01, 1.00009903e-02,\n",
       "        1.57404957e-02, 9.00006075e+00],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001878e+01, 1.00009903e-02,\n",
       "        1.71738046e-02, 1.10000759e+01],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001878e+01, 1.00009903e-02,\n",
       "        1.86071135e-02, 1.30000911e+01],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001877e+01, 1.00009902e-02,\n",
       "        2.00404238e-02, 1.50001063e+01],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001877e+01, 1.00009902e-02,\n",
       "        2.14737349e-02, 1.70001214e+01],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001876e+01, 1.00009902e-02,\n",
       "        2.29070460e-02, 1.90001366e+01],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001876e+01, 1.00009902e-02,\n",
       "        2.43403563e-02, 2.10001518e+01],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001875e+01, 1.00009902e-02,\n",
       "        2.57736674e-02, 2.30001669e+01],\n",
       "       [1.00000759e-01, 1.00000156e-01, 5.00001875e+01, 1.00009902e-02,\n",
       "        2.72069792e-02, 2.50001821e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001874e+01, 1.00009902e-02,\n",
       "        2.86402910e-02, 2.70001972e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001874e+01, 1.00009902e-02,\n",
       "        3.00736028e-02, 2.90002124e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001873e+01, 1.00009902e-02,\n",
       "        3.15069169e-02, 3.10002275e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001873e+01, 1.00009902e-02,\n",
       "        3.29402294e-02, 3.30002426e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001872e+01, 1.00009901e-02,\n",
       "        3.43735427e-02, 3.50002578e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001872e+01, 1.00009901e-02,\n",
       "        3.58068560e-02, 3.70002729e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001871e+01, 1.00009901e-02,\n",
       "        3.72401701e-02, 3.90002880e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001871e+01, 1.00009901e-02,\n",
       "        3.86734848e-02, 4.10003031e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001870e+01, 1.00009901e-02,\n",
       "        4.01068004e-02, 4.30003182e+01],\n",
       "       [1.00000758e-01, 1.00000155e-01, 5.00001870e+01, 1.00009901e-02,\n",
       "        4.15401159e-02, 4.50003333e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001869e+01, 1.00009901e-02,\n",
       "        4.29734321e-02, 4.70003485e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001869e+01, 1.00009901e-02,\n",
       "        4.44067484e-02, 4.90003636e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001868e+01, 1.00009901e-02,\n",
       "        4.58400646e-02, 5.10003787e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001868e+01, 1.00009901e-02,\n",
       "        4.72733816e-02, 5.30003937e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001867e+01, 1.00009900e-02,\n",
       "        4.87066979e-02, 5.50004088e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001867e+01, 1.00009900e-02,\n",
       "        5.01400141e-02, 5.70004239e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001866e+01, 1.00009900e-02,\n",
       "        5.15733304e-02, 5.90004390e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001866e+01, 1.00009900e-02,\n",
       "        5.30066481e-02, 6.10004541e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001865e+01, 1.00009900e-02,\n",
       "        5.44399658e-02, 6.30004692e+01],\n",
       "       [1.00000757e-01, 1.00000154e-01, 5.00001865e+01, 1.00009900e-02,\n",
       "        5.58732843e-02, 6.50004842e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001864e+01, 1.00009900e-02,\n",
       "        5.73066050e-02, 6.70004993e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001864e+01, 1.00009900e-02,\n",
       "        5.87399257e-02, 6.90005144e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001863e+01, 1.00009900e-02,\n",
       "        6.01732464e-02, 7.10005294e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001863e+01, 1.00009900e-02,\n",
       "        6.16065671e-02, 7.30005445e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001862e+01, 1.00009899e-02,\n",
       "        6.30398885e-02, 7.50005595e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001862e+01, 1.00009899e-02,\n",
       "        6.44732099e-02, 7.70005746e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001861e+01, 1.00009899e-02,\n",
       "        6.59065306e-02, 7.90005896e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001861e+01, 1.00009899e-02,\n",
       "        6.73398521e-02, 8.10006046e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001860e+01, 1.00009899e-02,\n",
       "        6.87731742e-02, 8.30006197e+01],\n",
       "       [1.00000756e-01, 1.00000153e-01, 5.00001860e+01, 1.00009899e-02,\n",
       "        7.02064964e-02, 8.50006347e+01],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001859e+01, 1.00009899e-02,\n",
       "        7.16398193e-02, 8.70006497e+01],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001859e+01, 1.00009899e-02,\n",
       "        7.30731429e-02, 8.90006648e+01],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001858e+01, 1.00009899e-02,\n",
       "        7.45064666e-02, 9.10006798e+01],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001858e+01, 1.00009899e-02,\n",
       "        7.59397902e-02, 9.30006948e+01],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001857e+01, 1.00009898e-02,\n",
       "        7.73731154e-02, 9.50007098e+01],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001857e+01, 1.00009898e-02,\n",
       "        7.88064397e-02, 9.70007248e+01],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001856e+01, 1.00009898e-02,\n",
       "        8.02397649e-02, 9.90007398e+01],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001856e+01, 1.00009898e-02,\n",
       "        8.16730892e-02, 1.01000755e+02],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001855e+01, 1.00009898e-02,\n",
       "        8.31064136e-02, 1.03000770e+02],\n",
       "       [1.00000755e-01, 1.00000152e-01, 5.00001855e+01, 1.00009898e-02,\n",
       "        8.45397387e-02, 1.05000785e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001854e+01, 1.00009898e-02,\n",
       "        8.59730639e-02, 1.07000800e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001854e+01, 1.00009898e-02,\n",
       "        8.74063912e-02, 1.09000815e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001853e+01, 1.00009898e-02,\n",
       "        8.88397178e-02, 1.11000830e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001853e+01, 1.00009898e-02,\n",
       "        9.02730444e-02, 1.13000845e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001852e+01, 1.00009897e-02,\n",
       "        9.17063717e-02, 1.15000860e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001852e+01, 1.00009897e-02,\n",
       "        9.31396998e-02, 1.17000875e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001851e+01, 1.00009897e-02,\n",
       "        9.45730257e-02, 1.19000890e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001851e+01, 1.00009897e-02,\n",
       "        9.60063522e-02, 1.21000905e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001850e+01, 1.00009897e-02,\n",
       "        9.74396788e-02, 1.23000920e+02],\n",
       "       [1.00000754e-01, 1.00000151e-01, 5.00001850e+01, 1.00009897e-02,\n",
       "        9.88730069e-02, 1.25000934e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001849e+01, 1.00009897e-02,\n",
       "        1.00306335e-01, 1.27000949e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001849e+01, 1.00009897e-02,\n",
       "        1.01739664e-01, 1.29000964e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001848e+01, 1.00009897e-02,\n",
       "        1.03172993e-01, 1.31000979e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001848e+01, 1.00009897e-02,\n",
       "        1.04606323e-01, 1.33000994e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001847e+01, 1.00009896e-02,\n",
       "        1.06039652e-01, 1.35001009e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001847e+01, 1.00009896e-02,\n",
       "        1.07472983e-01, 1.37001024e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001846e+01, 1.00009896e-02,\n",
       "        1.08906314e-01, 1.39001039e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001846e+01, 1.00009896e-02,\n",
       "        1.10339646e-01, 1.41001054e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001845e+01, 1.00009896e-02,\n",
       "        1.11772979e-01, 1.43001069e+02],\n",
       "       [1.00000753e-01, 1.00000150e-01, 5.00001845e+01, 1.00009896e-02,\n",
       "        1.13206313e-01, 1.45001084e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001844e+01, 1.00009896e-02,\n",
       "        1.14639645e-01, 1.47001099e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001844e+01, 1.00009896e-02,\n",
       "        1.16072978e-01, 1.49001114e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001843e+01, 1.00009896e-02,\n",
       "        1.17506312e-01, 1.51001128e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001843e+01, 1.00009896e-02,\n",
       "        1.18939646e-01, 1.53001143e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001842e+01, 1.00009895e-02,\n",
       "        1.20372980e-01, 1.55001158e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001842e+01, 1.00009895e-02,\n",
       "        1.21806314e-01, 1.57001173e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001841e+01, 1.00009895e-02,\n",
       "        1.23239648e-01, 1.59001188e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001841e+01, 1.00009895e-02,\n",
       "        1.24672983e-01, 1.61001203e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001840e+01, 1.00009895e-02,\n",
       "        1.26106319e-01, 1.63001218e+02],\n",
       "       [1.00000752e-01, 1.00000149e-01, 5.00001840e+01, 1.00009895e-02,\n",
       "        1.27539655e-01, 1.65001233e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001839e+01, 1.00009895e-02,\n",
       "        1.28972992e-01, 1.67001247e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001839e+01, 1.00009895e-02,\n",
       "        1.30406328e-01, 1.69001262e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001838e+01, 1.00009895e-02,\n",
       "        1.31839665e-01, 1.71001277e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001838e+01, 1.00009895e-02,\n",
       "        1.33273002e-01, 1.73001292e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001837e+01, 1.00009894e-02,\n",
       "        1.34706340e-01, 1.75001307e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001837e+01, 1.00009894e-02,\n",
       "        1.36139679e-01, 1.77001322e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001836e+01, 1.00009894e-02,\n",
       "        1.37573017e-01, 1.79001337e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001836e+01, 1.00009894e-02,\n",
       "        1.39006356e-01, 1.81001351e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001835e+01, 1.00009894e-02,\n",
       "        1.40439694e-01, 1.83001366e+02],\n",
       "       [1.00000751e-01, 1.00000148e-01, 5.00001835e+01, 1.00009894e-02,\n",
       "        1.41873032e-01, 1.85001381e+02],\n",
       "       [1.00000750e-01, 1.00000147e-01, 5.00001834e+01, 1.00009894e-02,\n",
       "        1.43306371e-01, 1.87001396e+02],\n",
       "       [1.00000750e-01, 1.00000147e-01, 5.00001834e+01, 1.00009894e-02,\n",
       "        1.44739710e-01, 1.89001411e+02],\n",
       "       [1.00000750e-01, 1.00000147e-01, 5.00001833e+01, 1.00009894e-02,\n",
       "        1.46173050e-01, 1.91001426e+02],\n",
       "       [1.00000750e-01, 1.00000147e-01, 5.00001833e+01, 1.00009894e-02,\n",
       "        1.47606390e-01, 1.93001440e+02],\n",
       "       [1.00000750e-01, 1.00000147e-01, 5.00001832e+01, 1.00009893e-02,\n",
       "        1.49039730e-01, 1.95001455e+02],\n",
       "       [1.00000750e-01, 1.00000147e-01, 5.00001832e+01, 1.00009893e-02,\n",
       "        1.50473069e-01, 1.97001470e+02],\n",
       "       [1.00000750e-01, 1.00000147e-01, 5.00001831e+01, 1.00009893e-02,\n",
       "        1.51906410e-01, 1.99001485e+02]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solution[0][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f015f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
