{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6dffaa",
   "metadata": {},
   "source": [
    "# Prediciting Evolutionary Dynamics of Microbial Systems with  Reinforcement Learnin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cae83b",
   "metadata": {},
   "source": [
    "## Overal View of the Algorithm\n",
    "\n",
    "![img](RLDFBA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4fc8d",
   "metadata": {},
   "source": [
    "### Step 1: Define toy models and the constants and load the toy networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ToyModel_SA = Model('Toy_Model')\n",
    "\n",
    "### S_Uptake ###\n",
    "\n",
    "S_Uptake = Reaction('Glc_Ex')\n",
    "S = Metabolite('Glc', compartment='c')\n",
    "S_Uptake.add_metabolites({S: -1})\n",
    "S_Uptake.lower_bound = -10\n",
    "S_Uptake.upper_bound = 0\n",
    "ToyModel_SA.add_reaction(S_Uptake)\n",
    "\n",
    "### ADP Production From Catabolism ###\n",
    "\n",
    "ATP_Cat = Reaction('ATP_Cat')\n",
    "ADP = Metabolite('ADP', compartment='c')\n",
    "ATP = Metabolite('ATP', compartment='c')\n",
    "S_x = Metabolite('S_x', compartment='c')\n",
    "ATP_Cat.add_metabolites({ADP: -1, S: -1, S_x: 1, ATP: 1})\n",
    "ATP_Cat.lower_bound = 0\n",
    "ATP_Cat.upper_bound = 1000\n",
    "ToyModel_SA.add_reaction(ATP_Cat)\n",
    "\n",
    "### ATP Maintenance ###\n",
    "\n",
    "ATP_M = Reaction('ATP_M')\n",
    "ATP_M.add_metabolites({ATP: -1, ADP: 1})\n",
    "ATP_M.lower_bound = 1\n",
    "ATP_M.upper_bound = 100\n",
    "ToyModel_SA.add_reaction(ATP_M)\n",
    "\n",
    "### Biomass Production ###\n",
    "\n",
    "X = Metabolite('X', compartment='c')\n",
    "X_Production = Reaction('X_Production')\n",
    "X_Production.add_metabolites({S_x: -1, ATP: -10, ADP: 10, X: 0.01})\n",
    "X_Production.lower_bound = 0\n",
    "X_Production.upper_bound = 1000\n",
    "ToyModel_SA.add_reaction(X_Production)\n",
    "\n",
    "### Biomass Release ###\n",
    "\n",
    "X_Release = Reaction('X_Ex')\n",
    "X_Release.add_metabolites({X: -1})\n",
    "X_Release.lower_bound = 0\n",
    "X_Release.upper_bound = 1000\n",
    "ToyModel_SA.add_reaction(X_Release)\n",
    "\n",
    "### Metabolism stuff ###\n",
    "\n",
    "P = Metabolite('P', compartment='c')\n",
    "P_Prod = Reaction('P_Prod')\n",
    "P_Prod.add_metabolites({S_x: -0.1, ATP: 1, ADP: -1, P: 0.1})\n",
    "P_Prod.lower_bound = 0\n",
    "P_Prod.upper_bound = 1000\n",
    "ToyModel_SA.add_reaction(P_Prod)\n",
    "\n",
    "### Product Release ###\n",
    "\n",
    "P_out = Reaction('P_Ex')\n",
    "P_out.add_metabolites({P: -1})\n",
    "P_out.lower_bound = 0\n",
    "P_out.upper_bound = 1000\n",
    "ToyModel_SA.add_reaction(P_out)\n",
    "ToyModel_SA.objective = 'X_Ex'\n",
    "\n",
    "### Amylase Production ###\n",
    "Amylase_Prod = Reaction('Amylase_Prod')\n",
    "Amylase = Metabolite('Amylase', compartment='c')\n",
    "Amylase_Prod.add_metabolites({P: -1, ATP: -20, ADP: 20, Amylase: 1})\n",
    "Amylase_Prod.lower_bound =0\n",
    "Amylase_Prod.upper_bound = 1000\n",
    "ToyModel_SA.add_reaction(Amylase_Prod)\n",
    "\n",
    "### Amylase Exchange ###\n",
    "Amylase_Ex = Reaction('Amylase_Ex')\n",
    "Amylase_Ex.add_metabolites({Amylase: -1})\n",
    "Amylase_Ex.lower_bound = 0\n",
    "Amylase_Ex.upper_bound = 1000\n",
    "ToyModel_SA.add_reaction(Amylase_Ex)\n",
    "\n",
    "ToyModel_SA.Biomass_Ind=4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### S_Uptake ###\n",
    "Toy_Model_NE_1 = Model('Toy_1')\n",
    "                       \n",
    "EX_S_sp1 = Reaction('EX_S_sp1')\n",
    "S = Metabolite('S', compartment='c')\n",
    "EX_S_sp1.add_metabolites({S: -1})\n",
    "EX_S_sp1.lower_bound = -10\n",
    "EX_S_sp1.upper_bound = 0\n",
    "Toy_Model_NE_1.add_reaction(EX_S_sp1)\n",
    "\n",
    "\n",
    "EX_A_sp1 = Reaction('EX_A_sp1')\n",
    "A = Metabolite('A', compartment='c')\n",
    "EX_A_sp1.add_metabolites({A: -1})\n",
    "EX_A_sp1.lower_bound = -100\n",
    "EX_A_sp1.upper_bound = 100\n",
    "Toy_Model_NE_1.add_reaction(EX_A_sp1)\n",
    "\n",
    "\n",
    "EX_B_sp1 = Reaction('EX_B_sp1')\n",
    "B = Metabolite('B', compartment='c')\n",
    "EX_B_sp1.add_metabolites({B: -1})\n",
    "EX_B_sp1.lower_bound = -100\n",
    "EX_B_sp1.upper_bound = 100\n",
    "Toy_Model_NE_1.add_reaction(EX_B_sp1)\n",
    "\n",
    "\n",
    "\n",
    "EX_P_sp1 = Reaction('EX_P_sp1')\n",
    "P = Metabolite('P', compartment='c')\n",
    "EX_P_sp1.add_metabolites({P:-1})\n",
    "EX_P_sp1.lower_bound = 0\n",
    "EX_P_sp1.upper_bound = 100\n",
    "Toy_Model_NE_1.add_reaction(EX_P_sp1)\n",
    "\n",
    "\n",
    "R_1_sp1 = Reaction('R_1_sp1')\n",
    "ADP = Metabolite('ADP', compartment='c')\n",
    "ATP = Metabolite('ATP', compartment='c')\n",
    "R_1_sp1.add_metabolites({ADP: -2, S: -1, P: 1, ATP: 2})\n",
    "R_1_sp1.lower_bound = 0\n",
    "R_1_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_1_sp1)\n",
    "\n",
    "\n",
    "R_2_sp1 = Reaction('R_2_sp1')\n",
    "R_2_sp1.add_metabolites({ADP: 1, P: -1, B: 3, ATP: -1})\n",
    "R_2_sp1.lower_bound = 0\n",
    "R_2_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_2_sp1)\n",
    "\n",
    "\n",
    "R_3_sp1 = Reaction('R_3_sp1')\n",
    "R_3_sp1.add_metabolites({ADP: 3, P: -1, A: 1, ATP: -3})\n",
    "R_3_sp1.lower_bound = 0\n",
    "R_3_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_3_sp1)\n",
    "\n",
    "\n",
    "\n",
    "R_4_sp1 = Reaction('R_4_sp1')\n",
    "R_4_sp1.add_metabolites({ADP:1 ,ATP: -1})\n",
    "R_4_sp1.lower_bound = 0\n",
    "R_4_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(R_4_sp1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OBJ_sp1 = Reaction(\"OBJ_sp1\")\n",
    "biomass_sp1 = Metabolite('biomass_sp1', compartment='c')\n",
    "OBJ_sp1.add_metabolites({ADP:5 ,ATP: -5,biomass_sp1:1,A:-1,B:-1})\n",
    "OBJ_sp1.lower_bound = 0\n",
    "OBJ_sp1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(OBJ_sp1)\n",
    "\n",
    "Biomass_1 = Reaction(\"Biomass_1\")\n",
    "Biomass_1.add_metabolites({biomass_sp1:-1})\n",
    "Biomass_1.lower_bound = 0\n",
    "Biomass_1.upper_bound = 1000\n",
    "Toy_Model_NE_1.add_reaction(Biomass_1)\n",
    "\n",
    "Toy_Model_NE_1.objective='Biomass_1'\n",
    "Toy_Model_NE_1.Biomass_Ind=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncore import write\n",
    "from cmath import inf\n",
    "from dataclasses import dataclass,field\n",
    "import datetime\n",
    "from tkinter import HIDDEN\n",
    "from xmlrpc.client import DateTime\n",
    "import numpy as np\n",
    "import cobra\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import pandas\n",
    "#import cplex\n",
    "# from ToyModel import  Toy_Model_NE_1,Toy_Model_NE_2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple,deque\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "import gym\n",
    "from tensorboardX import SummaryWriter\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Scaler=StandardScaler()\n",
    "HIDDEN_SIZE=20\n",
    "NUMBER_OF_BATCHES=100\n",
    "Main_dir = os.path.dirname(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77bfab",
   "metadata": {},
   "source": [
    "### Then we have to define a number of classes, objects, and functions that will be used during the simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21f2a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "        \n",
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        experience = (state, action, np.array([reward]), next_state)\n",
    "        self.buffer.appendleft(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            \n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "\n",
    "class DDPGActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGActor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 300),nn.Tanh(),\n",
    "            nn.Linear(300,300),nn.Tanh(),\n",
    "            nn.Linear(300, act_size),\n",
    "            \n",
    "             )\n",
    "\n",
    "    def forward(self, x):\n",
    "       return self.net(x)\n",
    "\n",
    "class DDPGCritic(nn.Module):       \n",
    "    \n",
    "    def __init__(self, obs_size, act_size):\n",
    "\n",
    "        super(DDPGCritic, self).__init__()\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 300),nn.Tanh(),\n",
    "            nn.Linear(300,300),nn.Tanh(),         \n",
    "            nn.Linear(300,300),nn.Tanh(),     \n",
    "            nn.Linear(300,20),\n",
    "            \n",
    "            )\n",
    "\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "                       nn.Linear(20 + act_size, 300),nn.Tanh(),\n",
    "                       nn.Linear(300,300),nn.Tanh(), \n",
    "                       nn.Linear(300, 1),\n",
    "                       )\n",
    "    \n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)           \n",
    "        return self.out_net(torch.cat([obs, a],dim=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87323fc8",
   "metadata": {},
   "source": [
    "### Definition of DFBA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e83dab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dFBA(Models, Mapping_Dict, Init_C, Params, t_span, dt=0.1):\n",
    "    \"\"\"\n",
    "    This function calculates the concentration of each species\n",
    "    Models is a list of COBRA Model objects\n",
    "    Mapping_Dict is a dictionary of dictionaries\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    # Initializing the ODE Solver\n",
    "    ##############################################################\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    ##############################################################\n",
    "    # Solving the ODE\n",
    "    ##############################################################\n",
    "    for m in Models:\n",
    "        m.episode_reward=0\n",
    "        m.episode_steps=[]\n",
    "\n",
    "    \n",
    "    sol, t = odeFwdEuler(ODE_System, Init_C, dt,  Params,\n",
    "                         t_span, Models, Mapping_Dict)\n",
    "    \n",
    "    for m in Models:\n",
    "        m.Episode=Episode(reward=m.episode_reward, steps=m.episode_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return [m.Episode for m in Models]\n",
    "\n",
    "def ODE_System(C, t, Models, Mapping_Dict, Params, dt,Counter):\n",
    "    \"\"\"\n",
    "    This function calculates the differential equations for the system\n",
    "    Models is a list of COBRA Model objects\n",
    "    NOTE: this implementation of DFBA is compatible with RL framework\n",
    "    Given a policy it will genrate episodes. Policies can be either deterministic or stochastic\n",
    "    Differential Equations Are Formatted as follows:\n",
    "    [0]-Models[1]\n",
    "    [1]-Models[2]\n",
    "    []-...\n",
    "    [n-1]-Models[n]\n",
    "    [n]-Exc[1]\n",
    "    [n+1]-Exc[2]\n",
    "    []-...\n",
    "    [n+m-1]-Exc[m]\n",
    "    [n+m]-Starch\n",
    "    \"\"\"\n",
    "    C[C < 0] = 0\n",
    "    dCdt = np.zeros(C.shape)\n",
    "    Sols = list([0 for i in range(Models.__len__())])\n",
    "    for i,M in enumerate(Models):\n",
    "        M.a=M.policy(torch.FloatTensor([C[M.observables]/Params[\"Env_States_Initial_MAX\"]])).detach().numpy()[0]\n",
    "        if random.random()<M.epsilon:\n",
    "            \n",
    "            # M.a=M.policy(torch.FloatTensor([C[M.observables]])).detach().numpy()[0]\n",
    "            # M.rand_act=np.random.uniform(low=-1, high=1,size=len(M.actions)).copy()\n",
    "            # M.a+=M.rand_act\n",
    "            M.a=np.random.uniform(low=-5,high=5,size=len(M.actions))\n",
    "            # M.a=np.random.uniform(low=-2, high=2,size=len(M.actions))\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        for index,item in enumerate(Mapping_Dict[\"Ex_sp\"]):\n",
    "            if Mapping_Dict['Mapping_Matrix'][index,i]!=-1:\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].upper_bound=100\n",
    "                M.reactions[Mapping_Dict['Mapping_Matrix'][index,i]].lower_bound=-General_Uptake_Kinetics(C[index+len(Models)])\n",
    "                \n",
    "            \n",
    "        for index,flux in enumerate(M.actions):\n",
    "\n",
    "            if M.a[index]<0:\n",
    "            \n",
    "                M.reactions[M.actions[index]].lower_bound=max(M.a[index],M.reactions[M.actions[index]].lower_bound)\n",
    "                # M.reactions[M.actions[index]].lower_bound=M.a[index]*M.reactions[M.actions[index]].lower_bound\n",
    "    \n",
    "            else:\n",
    "                M.reactions[M.actions[index]].lower_bound=min(M.a[index],M.reactions[M.actions[index]].upper_bound)\n",
    "                # M.reactions[M.actions[index]].lower_bound=M.a[index]*M.reactions[M.actions[index]].upper_bound\n",
    "            \n",
    "                # M.reactions[M.actions[index]].upper_bound=M.reactions[M.actions[index]].lower_bound+0.0000001\n",
    "        \n",
    "        Sols[i] = Models[i].optimize()\n",
    "\n",
    "        if Sols[i].status == 'infeasible':\n",
    "            Models[i].reward=0\n",
    "            dCdt[i] = 0\n",
    "            Sols[i].fluxes.iloc[:]=0\n",
    "\n",
    "        else:\n",
    "            dCdt[i] += Sols[i].objective_value*C[i]\n",
    "            Models[i].reward =Sols[i].objective_value\n",
    "        \n",
    "\n",
    "\n",
    "    ### Writing the balance equations\n",
    "\n",
    "    for i in range(Mapping_Dict[\"Mapping_Matrix\"].shape[0]):\n",
    "        for j in range(len(Models)):\n",
    "            if Mapping_Dict[\"Mapping_Matrix\"][i, j] != -1:\n",
    "                if Sols[j].status == 'infeasible':\n",
    "                    dCdt[i+len(Models)] += 0\n",
    "                else:\n",
    "                    dCdt[i+len(Models)] += Sols[j].fluxes.iloc[Mapping_Dict[\"Mapping_Matrix\"]\n",
    "                                                                    [i, j]]*C[j]\n",
    "    dCdt += np.array(Params[\"Dilution_Rate\"])*(Params[\"Inlet_C\"]-C)\n",
    "    Next_C=C+dCdt*dt\n",
    "    for m in Models:\n",
    "        m.buffer.push(torch.FloatTensor([C[M.observables]/Params[\"Env_States_Initial_MAX\"]]).detach().numpy()[0],m.a,m.reward,torch.FloatTensor([Next_C[M.observables]/Params[\"Env_States_Initial_MAX\"]]).detach().numpy()[0])\n",
    "        if Counter>0 and Counter%m.update_batch==0:\n",
    "            # TD_Error=[]\n",
    "            \n",
    "            S,A,R,Sp=m.buffer.sample(min(500,m.buffer.buffer.__len__()))\n",
    "            \n",
    "            Qvals = m.value(torch.FloatTensor(S), torch.FloatTensor(A))\n",
    "            next_actions = m.policy_target(torch.FloatTensor(Sp)).detach()\n",
    "            next_Q = m.value_target(torch.FloatTensor(Sp), next_actions)\n",
    "            Qprime = torch.FloatTensor(R) + next_Q\n",
    "            Qprime = torch.FloatTensor(R) +m.gamma*next_Q\n",
    "            critic_loss=m.Net_Obj(Qvals,Qprime.detach())\n",
    "            # m.R+=m.alpha*(torch.sum(Qprime-Qvals))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            m.optimizer_value.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            m.optimizer_value.step()\n",
    "            \n",
    "            policy_loss = -m.value(torch.FloatTensor(S), m.policy(torch.FloatTensor(S))).mean()\n",
    "            # m.R=m.alpha*torch.mean(Qvals-Qprime+torch.FloatTensor(R)-m.R).detach().numpy()\n",
    "            m.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            m.optimizer_policy.step()\n",
    "            \n",
    "        \n",
    "            for target_param, param in zip(m.policy_target.parameters(), m.policy.parameters()):\n",
    "                target_param.data.copy_(param.data * m.tau + target_param.data * (1-m.tau))\n",
    "        \n",
    "            for target_param, param in zip(m.value_target.parameters(), m.value.parameters()):\n",
    "                target_param.data.copy_(param.data * m.tau + target_param.data * (1-m.tau ))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        m.episode_reward+=m.reward\n",
    "\n",
    "    \n",
    "    \n",
    "    return dCdt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee6eab",
   "metadata": {},
   "source": [
    "### Now we need some utility functions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fd078ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Mapping_Matrix(Models):\n",
    "    \"\"\"\n",
    "    Given a list of COBRA model objects, this function will build a mapping matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Ex_sp = []\n",
    "    Temp_Map={}\n",
    "    for model in Models:\n",
    "        \n",
    "        \n",
    "        if not hasattr(model,\"Biomass_Ind\"):\n",
    "            raise Exception(\"Models must have 'Biomass_Ind' attribute in order for the DFBA to work properly!\")\n",
    "        \n",
    "        \n",
    "        for Ex_rxn in model.exchanges :\n",
    "            if Ex_rxn!=model.reactions[model.Biomass_Ind]:\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id not in Ex_sp:\n",
    "                    Ex_sp.append(list(Ex_rxn.metabolites.keys())[0].id)\n",
    "                if list(Ex_rxn.metabolites.keys())[0].id in Temp_Map.keys():\n",
    "                   Temp_Map[list(Ex_rxn.metabolites.keys())[0].id][model]=Ex_rxn\n",
    "                else:\n",
    "                     Temp_Map[list(Ex_rxn.metabolites.keys())[0].id]={model:Ex_rxn}\n",
    "\n",
    "    Mapping_Matrix = np.zeros((len(Ex_sp), len(Models)), dtype=int)\n",
    "    for i, id in enumerate(Ex_sp):\n",
    "        for j, model in enumerate(Models):\n",
    "            if model in Temp_Map[id].keys():\n",
    "                Mapping_Matrix[i, j] = model.reactions.index(Temp_Map[id][model].id)\n",
    "            else:\n",
    "                Mapping_Matrix[i, j] = -1\n",
    "    return {\"Ex_sp\": Ex_sp, \"Mapping_Matrix\": Mapping_Matrix}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Starch_Degradation_Kinetics(a_Amylase: float, Starch: float, Model=\"\", k: float = 1):\n",
    "    \"\"\"\n",
    "    This function calculates the rate of degradation of starch\n",
    "    a_Amylase Unit: mmol\n",
    "    Starch Unit: mg\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return a_Amylase*Starch*k/(Starch+10)\n",
    "\n",
    "\n",
    "def Glucose_Uptake_Kinetics(Glucose: float, Model=\"\"):\n",
    "    \"\"\"\n",
    "    This function calculates the rate of glucose uptake\n",
    "    ###It is just a simple imaginary model: Replace it with better model if necessary###\n",
    "    Glucose Unit: mmol\n",
    "\n",
    "    \"\"\"\n",
    "    return 1*(Glucose/(Glucose+20))\n",
    "\n",
    "\n",
    "def General_Uptake_Kinetics(Compound: float, Model=\"\"):\n",
    "    \"\"\"\n",
    "    This function calculates the rate of uptake of a compound in the reactor\n",
    "    ###It is just a simple imaginary model: Replace it with better model if necessary###\n",
    "    Compound Unit: mmol\n",
    "\n",
    "    \"\"\"\n",
    "    return 50*(Compound/(Compound+20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def odeFwdEuler(ODE_Function, ICs, dt, Params, t_span, Models, Mapping_Dict):\n",
    "    Integrator_Counter = 0\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    sol = np.zeros((len(t), len(ICs)))\n",
    "    sol[0] = ICs\n",
    "    for i in range(1, len(t)):\n",
    "        sol[i] = sol[i-1] + \\\n",
    "            ODE_Function(sol[i-1], t[i-1], Models, Mapping_Dict,\n",
    "                         Params, dt,Integrator_Counter)*dt\n",
    "        Integrator_Counter += 1\n",
    "    return sol, t\n",
    "\n",
    "\n",
    "def Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict,writer,t_span=[0, 100], dt=0.1):\n",
    "\n",
    "\n",
    "    Init_C[list(Params[\"Env_States\"])] = [random.uniform(Range[0], Range[1]) for Range in Params[\"Env_States_Initial_Ranges\"]]\n",
    "\n",
    "    \n",
    "    for BATCH in range(NUMBER_OF_BATCHES):\n",
    "        for model in Models:\n",
    "            model.epsilon=0.01+0.99/(np.exp(BATCH/10))\n",
    "            model.tau=0.01/(np.exp(BATCH/5))\n",
    "        dFBA(Models, Mapping_Dict, Init_C, Params, t_span, dt=dt)\n",
    "    \n",
    "        for mod in Models:\n",
    "            print(f\"{BATCH} - {mod.NAME} earned {mod.episode_reward} during this episode!\")\n",
    "            writer.add_scalar(f\"{mod.NAME} reward_mean\", mod.episode_reward, BATCH)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Flux_Clipper(Min,Number,Max):\n",
    "    return(min(max(Min,Number),Max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993c48a",
   "metadata": {},
   "source": [
    "### We now define the high level main function that controls every part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "015e3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(Models: list = [Toy_Model_NE_1.copy(), Toy_Model_NE_2.copy()], max_time: int = 100, Dil_Rate: float = 0.05, alpha: float = 0.01, Starting_Q: str = \"FBA\"):\n",
    "    \"\"\"\n",
    "    This is the main function for running dFBA.\n",
    "    The main requrement for working properly is\n",
    "    that the models use the same notation for the\n",
    "    same reactions.\n",
    "\n",
    "    Starting_Policy:\n",
    "\n",
    "    Defult --> Random: Initial Policy will be a random policy for all agents.\n",
    "    Otherwise --> a list of policies, pickle file addresses, for each agent.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Adding Agents info ###-----------------------------------------------------\n",
    "\n",
    "    # State dimensions in this RLDFBA variant include: [Agent1,...,Agentn, glucose,starch]\n",
    "    for i in range(len(Models)):\n",
    "        if not hasattr(Models[i], \"_name\"):\n",
    "            Models[i].NAME = \"Agent_\" + str(i)\n",
    "            print(f\"Agent {i} has been given a defult name\")\n",
    "        Models[i].solver.objective.name = \"_pfba_objective\"\n",
    "    # -------------------------------------------------------------------------------\n",
    "\n",
    "    # Mapping internal reactions to external reactions, and operational parameter\n",
    "    # setup ###-------------------------------------------------------------------\n",
    "\n",
    "    # For more information about the structure of the ODEs,see ODE_System function\n",
    "    # or the documentation.\n",
    "\n",
    "    Mapping_Dict = Build_Mapping_Matrix(Models)\n",
    "    Init_C = np.ones((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "    Inlet_C = np.zeros((len(Models)+len(Mapping_Dict[\"Ex_sp\"]),))\n",
    "    Inlet_C[2]=20\n",
    "    #Parameters that are use inside DFBA\n",
    "\n",
    "    Params = {\n",
    "        \"Dilution_Rate\": Dil_Rate,\n",
    "        \"Inlet_C\": Inlet_C,\n",
    "        \"Agents_Index\": [i for i in range(len(Models))],\n",
    "    }\n",
    "\n",
    "    #Define Agent attributes\n",
    "    Obs=[i for i in range(len(Models))]\n",
    "    Obs.extend([Mapping_Dict[\"Ex_sp\"].index(sp)+len(Models) for sp in Mapping_Dict[\"Ex_sp\"] if sp!='P' ])\n",
    "    for ind,m in enumerate(Models):\n",
    "        m.observables=Obs\n",
    "        m.actions=(Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"A\"),ind],Mapping_Dict[\"Mapping_Matrix\"][Mapping_Dict[\"Ex_sp\"].index(\"B\"),ind])\n",
    "        m.policy=DDPGActor(len(m.observables),len(m.actions))\n",
    "        m.policy_target=DDPGActor(len(m.observables),len(m.actions))\n",
    "        m.value=DDPGCritic(len(m.observables),len(m.actions))\n",
    "        m.value_target=DDPGCritic(len(m.observables),len(m.actions))\n",
    "        m.R=0\n",
    "        m.tau=0.005\n",
    "        m.optimizer_policy=optim.Adam(params=m.policy.parameters(), lr=0.01)\n",
    "        m.optimizer_policy_target=optim.Adam(params=m.policy.parameters(), lr=0.01)\n",
    "        m.optimizer_value=optim.Adam(params=m.value.parameters(), lr=0.01)\n",
    "        m.optimizer_value_target=optim.SGD(params=m.value.parameters(), lr=0.001)\n",
    "        m.Net_Obj=nn.MSELoss()\n",
    "        m.buffer=Memory(100000)\n",
    "        m.alpha=0.0001\n",
    "        m.update_batch=200\n",
    "        m.gamma=1\n",
    "        \n",
    "    ### I Assume that the environment states are all observable. Env states will be stochastic\n",
    "    Params[\"Env_States\"]=Models[0].observables\n",
    "    Params[\"Env_States_Initial_Ranges\"]=[[0.001,0.001+0.00000001],[0.001,0.001+0.00000001],[100,100+0.00001],[0,0],[0,0]]\n",
    "    Params[\"Env_States_Initial_MAX\"]=np.array([1,1,100,10,10])\n",
    "    for i in range(len(Models)):\n",
    "        Init_C[i] = 0.001\n",
    "        #Models[i].solver = \"cplex\"\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-DeepRLDFBA_NECOM\")\n",
    "    Outer_Counter = 0\n",
    "    \n",
    "\n",
    "    Generate_Batch(dFBA, Params, Init_C, Models, Mapping_Dict,writer)\n",
    "    \n",
    "    Time=datetime.datetime.now().strftime(\"%d_%m_%Y.%H_%M_%S\")\n",
    "    if not os.path.exists(os.path.join(Main_dir,\"Outputs\")):\n",
    "      os.mkdir(os.path.join(Main_dir,\"Outputs\"))\n",
    "    Results_Dir=os.path.join(Main_dir,\"Outputs\",str(Time))\n",
    "    os.mkdir(Results_Dir)\n",
    "    with open(os.path.join(Results_Dir,\"Models.pkl\"),'wb') as f:\n",
    "        pickle.dump(Models,f)\n",
    "    return Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2c42c",
   "metadata": {},
   "source": [
    "\n",
    "### Finally it's time to run!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd4b9a",
   "metadata": {},
   "source": [
    "## Case 1: Auxotrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1145da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 has been given a defult name\n",
      "Agent 1 has been given a defult name\n",
      "0 - Agent_0 earned 53.482206321042284 during this episode!\n",
      "0 - Agent_1 earned 3.950457601083324 during this episode!\n",
      "1 - Agent_0 earned 54.536559565035844 during this episode!\n",
      "1 - Agent_1 earned 3.78162028172261 during this episode!\n",
      "2 - Agent_0 earned 65.00410135247253 during this episode!\n",
      "2 - Agent_1 earned 9.082702417866688 during this episode!\n",
      "3 - Agent_0 earned 72.67974120144576 during this episode!\n",
      "3 - Agent_1 earned 22.375112076632742 during this episode!\n",
      "4 - Agent_0 earned 78.24226012353793 during this episode!\n",
      "4 - Agent_1 earned 30.76309576336569 during this episode!\n",
      "5 - Agent_0 earned 74.51820044730222 during this episode!\n",
      "5 - Agent_1 earned 21.920507149538746 during this episode!\n",
      "6 - Agent_0 earned 80.18356528565842 during this episode!\n",
      "6 - Agent_1 earned 31.241542431775986 during this episode!\n",
      "7 - Agent_0 earned 97.54341011673338 during this episode!\n",
      "7 - Agent_1 earned 52.54295455146542 during this episode!\n",
      "8 - Agent_0 earned 85.44908220913493 during this episode!\n",
      "8 - Agent_1 earned 35.68414083742473 during this episode!\n",
      "9 - Agent_0 earned 78.24780997040433 during this episode!\n",
      "9 - Agent_1 earned 20.20425506842983 during this episode!\n",
      "10 - Agent_0 earned 124.52618157138045 during this episode!\n",
      "10 - Agent_1 earned 86.82363326103658 during this episode!\n",
      "11 - Agent_0 earned 97.39344382661055 during this episode!\n",
      "11 - Agent_1 earned 49.62022498416193 during this episode!\n",
      "12 - Agent_0 earned 93.20627091057112 during this episode!\n",
      "12 - Agent_1 earned 40.02546957637406 during this episode!\n",
      "13 - Agent_0 earned 88.22048656945577 during this episode!\n",
      "13 - Agent_1 earned 30.657078831411905 during this episode!\n",
      "14 - Agent_0 earned 121.31224419090773 during this episode!\n",
      "14 - Agent_1 earned 82.80213290832317 during this episode!\n",
      "15 - Agent_0 earned 81.81166232124997 during this episode!\n",
      "15 - Agent_1 earned 22.239445344847827 during this episode!\n",
      "16 - Agent_0 earned 84.33677228112101 during this episode!\n",
      "16 - Agent_1 earned 27.335863782055124 during this episode!\n",
      "17 - Agent_0 earned 88.47707178261848 during this episode!\n",
      "17 - Agent_1 earned 31.086328953568902 during this episode!\n",
      "18 - Agent_0 earned 87.50213105350845 during this episode!\n",
      "18 - Agent_1 earned 29.739736648314892 during this episode!\n",
      "19 - Agent_0 earned 77.36012014106187 during this episode!\n",
      "19 - Agent_1 earned 10.420220999733202 during this episode!\n",
      "20 - Agent_0 earned 83.25982846870788 during this episode!\n",
      "20 - Agent_1 earned 20.851915502857818 during this episode!\n",
      "21 - Agent_0 earned 79.89891862867775 during this episode!\n",
      "21 - Agent_1 earned 15.053820118585014 during this episode!\n",
      "22 - Agent_0 earned 84.62331660883706 during this episode!\n",
      "22 - Agent_1 earned 22.87132654794709 during this episode!\n",
      "23 - Agent_0 earned 78.58191491827716 during this episode!\n",
      "23 - Agent_1 earned 11.734329915784762 during this episode!\n",
      "24 - Agent_0 earned 78.98216860040554 during this episode!\n",
      "24 - Agent_1 earned 12.216670324172709 during this episode!\n",
      "25 - Agent_0 earned 80.2608603588668 during this episode!\n",
      "25 - Agent_1 earned 16.04916810503986 during this episode!\n",
      "26 - Agent_0 earned 81.12827930437096 during this episode!\n",
      "26 - Agent_1 earned 15.451172048727862 during this episode!\n",
      "27 - Agent_0 earned 76.73042703894393 during this episode!\n",
      "27 - Agent_1 earned 7.152870605618593 during this episode!\n",
      "28 - Agent_0 earned 78.97114782096992 during this episode!\n",
      "28 - Agent_1 earned 10.832015023039089 during this episode!\n",
      "29 - Agent_0 earned 76.79392819082527 during this episode!\n",
      "29 - Agent_1 earned 8.084009746901577 during this episode!\n",
      "30 - Agent_0 earned 74.27777350000225 during this episode!\n",
      "30 - Agent_1 earned 6.906566925589395 during this episode!\n",
      "31 - Agent_0 earned 69.56179018834067 during this episode!\n",
      "31 - Agent_1 earned 4.603608615261604 during this episode!\n",
      "32 - Agent_0 earned 64.52722947517321 during this episode!\n",
      "32 - Agent_1 earned 7.539223070812194 during this episode!\n",
      "33 - Agent_0 earned 55.817745143143654 during this episode!\n",
      "33 - Agent_1 earned 5.403669445606886 during this episode!\n",
      "34 - Agent_0 earned 6.15868500426533 during this episode!\n",
      "34 - Agent_1 earned 0.10297591540354009 during this episode!\n",
      "35 - Agent_0 earned 5.661609220378508 during this episode!\n",
      "35 - Agent_1 earned 0.12924042918931555 during this episode!\n",
      "36 - Agent_0 earned 7.2696229955599945 during this episode!\n",
      "36 - Agent_1 earned 0.18461906701265132 during this episode!\n",
      "37 - Agent_0 earned 3.3295981364518195 during this episode!\n",
      "37 - Agent_1 earned 0.08292112951872005 during this episode!\n",
      "38 - Agent_0 earned 3.287576757718631 during this episode!\n",
      "38 - Agent_1 earned 0.14609664974725914 during this episode!\n",
      "39 - Agent_0 earned 8.04196794423213 during this episode!\n",
      "39 - Agent_1 earned 0.2153948310429466 during this episode!\n",
      "40 - Agent_0 earned 2.9453356886443034 during this episode!\n",
      "40 - Agent_1 earned 0.05884207026692447 during this episode!\n",
      "41 - Agent_0 earned 4.131707184678006 during this episode!\n",
      "41 - Agent_1 earned 0.05185627187525164 during this episode!\n",
      "42 - Agent_0 earned 1.5850364824038277 during this episode!\n",
      "42 - Agent_1 earned 0.007503856538596523 during this episode!\n",
      "43 - Agent_0 earned 1.3398318918523724 during this episode!\n",
      "43 - Agent_1 earned 0.00016547492907328378 during this episode!\n",
      "44 - Agent_0 earned 2.5402486717061468 during this episode!\n",
      "44 - Agent_1 earned 4.4764192352886304e-15 during this episode!\n",
      "45 - Agent_0 earned 4.1054698398024385 during this episode!\n",
      "45 - Agent_1 earned 3.8576855738572394e-05 during this episode!\n",
      "46 - Agent_0 earned 1.1282042790256384 during this episode!\n",
      "46 - Agent_1 earned 5.200002246370516e-05 during this episode!\n",
      "47 - Agent_0 earned 2.703011908268893 during this episode!\n",
      "47 - Agent_1 earned 7.070104155587842e-05 during this episode!\n",
      "48 - Agent_0 earned 3.79984245504239 during this episode!\n",
      "48 - Agent_1 earned 0.0004149860212803524 during this episode!\n",
      "49 - Agent_0 earned 3.4891345879769853 during this episode!\n",
      "49 - Agent_1 earned 1.4959606851778773e-05 during this episode!\n",
      "50 - Agent_0 earned 4.548398702622267 during this episode!\n",
      "50 - Agent_1 earned 1.0335469686495458e-05 during this episode!\n",
      "51 - Agent_0 earned 2.2626548058382094 during this episode!\n",
      "51 - Agent_1 earned 1.9184653865522706e-15 during this episode!\n",
      "52 - Agent_0 earned 1.9325075697015135 during this episode!\n",
      "52 - Agent_1 earned 3.508326693413722e-05 during this episode!\n",
      "53 - Agent_0 earned 1.7192339356026585 during this episode!\n",
      "53 - Agent_1 earned 0.00010139047067696084 during this episode!\n",
      "54 - Agent_0 earned 0.9122557804560102 during this episode!\n",
      "54 - Agent_1 earned 1.7053025658242404e-15 during this episode!\n",
      "55 - Agent_0 earned 2.806988728679876 during this episode!\n",
      "55 - Agent_1 earned 7.074239917283622e-05 during this episode!\n",
      "56 - Agent_0 earned 1.5370316055491509 during this episode!\n",
      "56 - Agent_1 earned 1.3201935379250073e-05 during this episode!\n",
      "57 - Agent_0 earned 2.2584206459484704 during this episode!\n",
      "57 - Agent_1 earned 0.0001317048441999437 during this episode!\n",
      "58 - Agent_0 earned 1.8963094111178114 during this episode!\n",
      "58 - Agent_1 earned 1.7370490239720866e-05 during this episode!\n",
      "59 - Agent_0 earned 0.28659719014328267 during this episode!\n",
      "59 - Agent_1 earned 3.602726139200829e-05 during this episode!\n",
      "60 - Agent_0 earned 1.040846988748398 during this episode!\n",
      "60 - Agent_1 earned 4.208785655599398e-05 during this episode!\n",
      "61 - Agent_0 earned 0.2927387794248156 during this episode!\n",
      "61 - Agent_1 earned 1.6914001676133364e-05 during this episode!\n",
      "62 - Agent_0 earned 0.8148828251898841 during this episode!\n",
      "62 - Agent_1 earned 0.0 during this episode!\n",
      "63 - Agent_0 earned 1.0177619785791774 during this episode!\n",
      "63 - Agent_1 earned 5.338727139318528e-05 during this episode!\n",
      "64 - Agent_0 earned 1.671169546289202 during this episode!\n",
      "64 - Agent_1 earned 2.87848290462666e-05 during this episode!\n",
      "65 - Agent_0 earned 1.354649108173455 during this episode!\n",
      "65 - Agent_1 earned 5.7831776058966156e-05 during this episode!\n",
      "66 - Agent_0 earned 1.547900753703979 during this episode!\n",
      "66 - Agent_1 earned 9.937047620358201e-05 during this episode!\n",
      "67 - Agent_0 earned 1.780557278023831 during this episode!\n",
      "67 - Agent_1 earned 3.123871020924716e-06 during this episode!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 - Agent_0 earned 0.30206258693940613 during this episode!\n",
      "68 - Agent_1 earned 9.690091606112433e-05 during this episode!\n",
      "69 - Agent_0 earned 1.0989283399466767 during this episode!\n",
      "69 - Agent_1 earned 7.329070342135012e-05 during this episode!\n",
      "70 - Agent_0 earned 1.454500118445469 during this episode!\n",
      "70 - Agent_1 earned 3.1678017972315026e-05 during this episode!\n",
      "71 - Agent_0 earned 2.6899517179081367 during this episode!\n",
      "71 - Agent_1 earned 1.205886881345083e-06 during this episode!\n",
      "72 - Agent_0 earned 2.139887964861039 during this episode!\n",
      "72 - Agent_1 earned 9.620136437894469e-07 during this episode!\n",
      "73 - Agent_0 earned 0.5012356548447354 during this episode!\n",
      "73 - Agent_1 earned 2.5107967204462875e-05 during this episode!\n",
      "74 - Agent_0 earned 2.358398020129947 during this episode!\n",
      "74 - Agent_1 earned 5.718712199318993e-05 during this episode!\n",
      "75 - Agent_0 earned 0.2724774268072005 during this episode!\n",
      "75 - Agent_1 earned 3.397799098594236e-05 during this episode!\n",
      "76 - Agent_0 earned 1.141420848183116 during this episode!\n",
      "76 - Agent_1 earned 2.290102583249207e-05 during this episode!\n",
      "77 - Agent_0 earned 3.032171468346464 during this episode!\n",
      "77 - Agent_1 earned 0.00011192722394383894 during this episode!\n",
      "78 - Agent_0 earned 1.3655938257510578 during this episode!\n",
      "78 - Agent_1 earned 3.5244471681164325e-05 during this episode!\n",
      "79 - Agent_0 earned 0.24714454296631788 during this episode!\n",
      "79 - Agent_1 earned 3.111673982546959e-05 during this episode!\n",
      "80 - Agent_0 earned 0.4292615668366059 during this episode!\n",
      "80 - Agent_1 earned 4.264951399218609e-05 during this episode!\n",
      "81 - Agent_0 earned 0.33964171577753177 during this episode!\n",
      "81 - Agent_1 earned 1.4729696545262215e-06 during this episode!\n",
      "82 - Agent_0 earned 1.2844335337145278 during this episode!\n",
      "82 - Agent_1 earned 0.0 during this episode!\n",
      "83 - Agent_0 earned 2.821467932396795 during this episode!\n",
      "83 - Agent_1 earned 8.017326792070909e-07 during this episode!\n",
      "84 - Agent_0 earned 1.9525794691980034 during this episode!\n",
      "84 - Agent_1 earned 5.9158455171606974e-05 during this episode!\n",
      "85 - Agent_0 earned 0.01432266991476988 during this episode!\n",
      "85 - Agent_1 earned 0.00010351978682106801 during this episode!\n",
      "86 - Agent_0 earned 0.001259978196682321 during this episode!\n",
      "86 - Agent_1 earned 0.0 during this episode!\n",
      "87 - Agent_0 earned 0.009502437486087235 during this episode!\n",
      "87 - Agent_1 earned 0.0017488591166043431 during this episode!\n",
      "88 - Agent_0 earned 0.000788283527094234 during this episode!\n",
      "88 - Agent_1 earned 0.0 during this episode!\n",
      "89 - Agent_0 earned 0.0 during this episode!\n",
      "89 - Agent_1 earned 5.900585654842188e-05 during this episode!\n",
      "90 - Agent_0 earned 0.00023528771595644345 during this episode!\n",
      "90 - Agent_1 earned 3.398123677845089e-05 during this episode!\n",
      "91 - Agent_0 earned 0.0003192803298343044 during this episode!\n",
      "91 - Agent_1 earned 0.0007897330925114062 during this episode!\n",
      "92 - Agent_0 earned 1.313290887463875e-05 during this episode!\n",
      "92 - Agent_1 earned 0.0002038569816088026 during this episode!\n",
      "93 - Agent_0 earned 0.0009031209056551299 during this episode!\n",
      "93 - Agent_1 earned 8.374324836660811e-05 during this episode!\n",
      "94 - Agent_0 earned 0.0007087298393103889 during this episode!\n",
      "94 - Agent_1 earned 8.72770255971635e-05 during this episode!\n",
      "95 - Agent_0 earned 1.657487404693736e-05 during this episode!\n",
      "95 - Agent_1 earned 0.0003749545222426277 during this episode!\n",
      "96 - Agent_0 earned 0.00017204116961950199 during this episode!\n",
      "96 - Agent_1 earned 0.0008849503962993678 during this episode!\n",
      "97 - Agent_0 earned 0.00029949452077473527 during this episode!\n",
      "97 - Agent_1 earned 0.0005749595034653483 during this episode!\n",
      "98 - Agent_0 earned 0.000748751776579931 during this episode!\n",
      "98 - Agent_1 earned 1.556273073440773e-05 during this episode!\n",
      "99 - Agent_0 earned 0.0 during this episode!\n",
      "99 - Agent_1 earned 2.8850181812791432e-05 during this episode!\n"
     ]
    }
   ],
   "source": [
    "Toy_Model_NE_1_A=Toy_Model_NE_1.copy()\n",
    "Toy_Model_NE_2_A=Toy_Model_NE_2.copy()\n",
    "Toy_Model_NE_1_A.remove_reactions('R_2_sp1')\n",
    "Toy_Model_NE_1_A.Biomass_Ind=8\n",
    "Toy_Model_NE_2_A.remove_reactions('R_3_sp2')\n",
    "Toy_Model_NE_2_A.Biomass_Ind=8\n",
    "Models_Auxotrophy=[Toy_Model_NE_1_A,Toy_Model_NE_2_A]\n",
    "Models_A=main(Models_Auxotrophy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f44de5",
   "metadata": {},
   "source": [
    "## Step n: Plotting the policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91249a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Space_Size=10000\n",
    "Space=np.random.uniform(low=[0,0,0,0,0],high=[1,1,1,1,1],size=(Space_Size,5))\n",
    "Actions_1=Models_A[0].policy(torch.FloatTensor(Space))\n",
    "Actions_2=Models_A[1].policy(torch.FloatTensor(Space))\n",
    "DF_1_A=pd.DataFrame(np.hstack((Space,Actions_1.detach().numpy())),columns=[\"Agent1\",\"Agent2\",\"S\",\"A\",\"B\",\"A_Export\",\"B_Export\"])\n",
    "DF_2_A=pd.DataFrame(np.hstack((Space,Actions_2.detach().numpy())),columns=[\"Agent1\",\"Agent2\",\"S\",\"A\",\"B\",\"A_Export\",\"B_Export\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ffcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure(data=[go.Scatter3d(x=DF_1_A[\"A\"], y=DF_1_A[\"B\"],z=DF_1_A[\"B_Export\"],mode='markers',\n",
    "#                                        marker=dict(\n",
    "#         size=5,\n",
    "#         color=DF_1_A[\"B_Export\"],                # set color to an array/list of desired values\n",
    "#         colorscale='Viridis',   # choose a colorscale\n",
    "#         opacity=1,\n",
    "#         colorbar=dict(thickness=10,title=\"B Export\")),\n",
    "# )])\n",
    "# fig.update_layout(scene = dict(\n",
    "#                     xaxis_title='A[C]',\n",
    "#                     yaxis_title='B[C]',\n",
    "#                     zaxis_title='B Export'),\n",
    "#                     width=600,\n",
    "#                     margin=dict(r=20, b=10, l=10, t=10))\n",
    "\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
